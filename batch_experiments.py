"""
æ‰¹é‡å®éªŒè„šæœ¬
ç”¨äºè¿è¡Œå¤šä¸ªå¿ƒç‡ä¼°è®¡æ–¹æ³•å’Œçª—å£é•¿åº¦çš„ç»„åˆå®éªŒ

è¿è¡Œæ–¹å¼ï¼š
    python batch_experiments.py
"""
"""
ç›¸æ¯”v1å¢åŠ è·ç¦»ç»´åº¦åˆ†æ
ç›¸æ¯”v2å¢åŠ å››å¼ å›¾åˆæˆä¸€å¼ å¤§å›¾ï¼Œä¸”ä¿®æ”¹å­—å·å¤§å°
"""
import os
import sys
import json
import time
import logging
from datetime import datetime
from typing import Dict, List
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

# ----------------------------------------------------------------------------
# âš ï¸ å‡è®¾æ”¯æŒå­˜åœ¨
# å‡è®¾å­˜åœ¨ main.py ä¸”å…¶ä¸­å®šä¹‰äº† run_experiment å‡½æ•°
# å‡è®¾å­˜åœ¨ config æ¨¡å—
# ----------------------------------------------------------------------------
# å¯¼å…¥ä¸»ç¨‹åº (å‡è®¾ main.py å’Œ config å­˜åœ¨)
try:
    from main import run_experiment
    import config
except ImportError:
    # å¦‚æœç¯å¢ƒä¸å®Œæ•´ï¼Œå®šä¹‰ä¸€ä¸ª mock å‡½æ•°ä»¥ä¿æŒä»£ç ç»“æ„å®Œæ•´æ€§
    print("Warning: main.run_experiment or config not found. Using mock implementation.")
    def run_experiment(hr_method, loc_method, window_s, output_dir):
        """Mock implementation for run_experiment"""
        time.sleep(1) # Simulate computation time
        
        # Simulate evaluation results
        np.random.seed(hash(f"{hr_method}_{window_s}") % (2**32))
        base_mae = 3.0 + np.random.randn() * 0.5 - 0.02 * window_s / 30.0
        mae = max(0.5, base_mae)
        rmse = mae * 1.3
        
        evaluation_results = {
            'visualization': {
                'bland_altman': {
                    'loa_range': rmse * 3.92,
                    'mean_difference': np.random.randn() * 0.5
                }
            },
            'segment': {
                'MAE': mae,
                'RMSE': rmse,
                'Correlation': min(0.99, max(0.85, 1 - mae / 15)),
                'n_samples': 300
            }
        }
        
        # Simulate computation times
        computation_times = [np.random.rand() * 0.1 for _ in range(300)]
        
        # Mock file generation for distance analysis (required by ResultAnalyzer)
        os.makedirs(output_dir, exist_ok=True)
        
        # Mock results file with distance info
        distances = [40, 50, 60] * 100 
        radar_hr = np.random.uniform(60, 100, 300)
        neulog_hr = radar_hr + np.random.randn(300) * mae
        
        mock_df = pd.DataFrame({
            'distance': distances[:300],
            'radar_hr_bpm': radar_hr,
            'neulog_hr_count': neulog_hr
        })
        mock_df.to_csv(os.path.join(output_dir, 'heart_rate_results.csv'), index=False)
        
        return evaluation_results, computation_times

# ****************************************************************************
# ã€ä¿®æ”¹ï¼šå­—ä½“å¤§å°è®¾ç½®ã€‘
# ****************************************************************************

# è®¾ç½®matplotlibä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False
plt.rcParams['figure.dpi'] = 300

# ç»Ÿä¸€è®¾ç½®å­—ä½“å¤§å°
FONT_SIZE_AXIS = 18
FONT_SIZE_TITLE = 20
FONT_SIZE_LEGEND = 16

plt.rcParams['axes.labelsize'] = FONT_SIZE_AXIS
plt.rcParams['xtick.labelsize'] = FONT_SIZE_AXIS
plt.rcParams['ytick.labelsize'] = FONT_SIZE_AXIS
plt.rcParams['legend.fontsize'] = FONT_SIZE_LEGEND
plt.rcParams['axes.titlesize'] = FONT_SIZE_TITLE     # Axes title size
plt.rcParams['figure.titlesize'] = FONT_SIZE_TITLE   # Figure suptitle size

# ============================================================================
# ã€å®éªŒé…ç½®ã€‘
# ============================================================================

# å®éªŒçŸ©é˜µï¼šæ¯ä¸ªæ–¹æ³•å¯¹åº”çš„çª—å£é•¿åº¦åˆ—è¡¨
# EXPERIMENT_MATRIX = {
#     'fft': [30, 60, 90, 120],
#     'stft': [30, 60, 90, 120],
#     'wavelet': [30, 60, 90, 120],
#     'dct': [30, 60, 90, 120],
#     'emd': [30, 60, 90, 120],
#     'eemd': [30, 60, 90, 120],
#     'vmd': [30, 60, 90, 120]
# }

EXPERIMENT_MATRIX = {
    'wavelet': [30, 60, 90, 120],
    'vmd': [30, 60, 90, 120]
}

# äººä½“å®šä½æ–¹æ³•ï¼ˆå›ºå®šä½¿ç”¨manualï¼‰
LOCALIZATION_METHOD = 'manual'

# è¾“å‡ºç›®å½•
BATCH_OUTPUT_DIR = './batch_results'
CACHE_DIR = os.path.join(BATCH_OUTPUT_DIR, 'cache')
FIGURES_DIR = os.path.join(BATCH_OUTPUT_DIR, 'figures')
TABLES_DIR = os.path.join(BATCH_OUTPUT_DIR, 'tables')

# ============================================================================
# ã€æ—¥å¿—é…ç½®ã€‘
# ============================================================================

def setup_logging():
    """è®¾ç½®æ—¥å¿—"""
    os.makedirs(BATCH_OUTPUT_DIR, exist_ok=True)
    
    log_file = os.path.join(BATCH_OUTPUT_DIR, f'batch_experiments_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log')
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file, encoding='utf-8'),
            logging.StreamHandler(sys.stdout)
        ]
    )
    
    return logging.getLogger(__name__)

# ============================================================================
# ã€å®éªŒè¿è¡Œå™¨ã€‘
# ============================================================================

class BatchExperimentRunner:
    """æ‰¹é‡å®éªŒè¿è¡Œå™¨"""
    
    def __init__(self, experiment_matrix: Dict[str, List[int]], logger: logging.Logger):
        """
        åˆå§‹åŒ–æ‰¹é‡å®éªŒè¿è¡Œå™¨
        """
        self.experiment_matrix = experiment_matrix
        self.logger = logger
        self.results_cache = {}
        
        # åˆ›å»ºå¿…è¦çš„ç›®å½•
        os.makedirs(BATCH_OUTPUT_DIR, exist_ok=True)
        os.makedirs(CACHE_DIR, exist_ok=True)
        os.makedirs(FIGURES_DIR, exist_ok=True)
        os.makedirs(TABLES_DIR, exist_ok=True)
        
        # åŠ è½½å·²æœ‰çš„ç¼“å­˜
        self._load_cache()
    
    def _load_cache(self):
        """åŠ è½½å·²æœ‰çš„ç¼“å­˜ç»“æœ"""
        cache_file = os.path.join(CACHE_DIR, 'results_cache.json')
        if os.path.exists(cache_file):
            try:
                with open(cache_file, 'r', encoding='utf-8') as f:
                    self.results_cache = json.load(f)
                self.logger.info(f"âœ… åŠ è½½äº† {len(self.results_cache)} ä¸ªç¼“å­˜ç»“æœ")
            except Exception as e:
                self.logger.warning(f"âš ï¸ åŠ è½½ç¼“å­˜å¤±è´¥: {e}")
                self.results_cache = {}
        else:
            self.results_cache = {}
    
    def _save_cache(self):
        """ä¿å­˜ç¼“å­˜ç»“æœ"""
        cache_file = os.path.join(CACHE_DIR, 'results_cache.json')
        try:
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(self.results_cache, f, indent=4, ensure_ascii=False)
            self.logger.info(f"âœ… ä¿å­˜äº† {len(self.results_cache)} ä¸ªç¼“å­˜ç»“æœ")
        except Exception as e:
            self.logger.error(f"âŒ ä¿å­˜ç¼“å­˜å¤±è´¥: {e}")
    
    def _get_experiment_key(self, method: str, window: int) -> str:
        """ç”Ÿæˆå®éªŒçš„å”¯ä¸€æ ‡è¯†"""
        return f"{method}_{window}s"
    
    def _run_single_experiment(self, method: str, window: int) -> Dict:
        """
        è¿è¡Œå•ä¸ªå®éªŒ
        """
        exp_key = self._get_experiment_key(method, window)
        
        # æ£€æŸ¥ç¼“å­˜
        if exp_key in self.results_cache:
            self.logger.info(f" Â âš¡ ä½¿ç”¨ç¼“å­˜: {exp_key}")
            return self.results_cache[exp_key]
        
        # è¿è¡Œå®éªŒ
        self.logger.info(f" Â ğŸš€ è¿è¡Œå®éªŒ: {exp_key}")
        
        output_dir = os.path.join(CACHE_DIR, exp_key)
        
        try:
            # è°ƒç”¨ä¸»ç¨‹åº
            start_time = time.time()
            evaluation_results, computation_times = run_experiment(
                hr_method=method,
                loc_method=LOCALIZATION_METHOD,
                window_s=window,
                output_dir=output_dir
            )
            total_time = time.time() - start_time
            
            # æå–å…³é”®æŒ‡æ ‡
            result_dict = {
                'method': method,
                'window_s': window,
                'loa_range': evaluation_results['visualization']['bland_altman']['loa_range'],
                'mean_difference': evaluation_results['visualization']['bland_altman']['mean_difference'],
                'mae': evaluation_results['segment']['MAE'],
                'rmse': evaluation_results['segment']['RMSE'],
                'correlation': evaluation_results['segment']['Correlation'],
                'n_samples': evaluation_results['segment']['n_samples'],
                'mean_computation_time_per_segment_ms': np.mean(computation_times) * 1000,  # è½¬æ¢ä¸ºæ¯«ç§’
                'std_computation_time_per_segment_ms': np.std(computation_times) * 1000,
                'total_experiment_time_s': total_time,
                'timestamp': datetime.now().isoformat()
            }
            
            # ç¼“å­˜ç»“æœ
            self.results_cache[exp_key] = result_dict
            self._save_cache()
            
            self.logger.info(f" Â âœ… å®Œæˆ: {exp_key} | MAE={result_dict['mae']:.2f} | "
                             f"æ—¶é—´={result_dict['mean_computation_time_per_segment_ms']:.2f}ms")
            
            return result_dict
            
        except Exception as e:
            self.logger.error(f" Â âŒ å®éªŒå¤±è´¥: {exp_key} | é”™è¯¯: {e}")
            import traceback
            self.logger.error(traceback.format_exc())
            return None
    
    def run_all_experiments(self) -> pd.DataFrame:
        """
        è¿è¡Œæ‰€æœ‰å®éªŒ
        """
        self.logger.info("=" * 70)
        self.logger.info("ğŸš€ å¼€å§‹æ‰¹é‡å®éªŒ")
        self.logger.info("=" * 70)
        
        # è®¡ç®—æ€»å®éªŒæ•°
        total_experiments = sum(len(windows) for windows in self.experiment_matrix.values())
        self.logger.info(f"æ€»å®éªŒæ•°: {total_experiments}")
        
        # è¿è¡Œæ‰€æœ‰å®éªŒ
        all_results = []
        
        with tqdm(total=total_experiments, desc="æ€»è¿›åº¦") as pbar:
            for method, windows in self.experiment_matrix.items():
                self.logger.info(f"\n{'='*70}")
                self.logger.info(f"æ–¹æ³•: {method.upper()} | çª—å£æ•°: {len(windows)}")
                self.logger.info(f"{'='*70}")
                
                for window in windows:
                    result = self._run_single_experiment(method, window)
                    if result is not None:
                        all_results.append(result)
                    pbar.update(1)
        
        # è½¬æ¢ä¸ºDataFrame
        results_df = pd.DataFrame(all_results)
        
        # ä¿å­˜å®Œæ•´ç»“æœ
        results_file = os.path.join(BATCH_OUTPUT_DIR, 'all_results.csv')
        results_df.to_csv(results_file, index=False, encoding='utf-8-sig')
        self.logger.info(f"\nâœ… æ‰€æœ‰ç»“æœå·²ä¿å­˜è‡³: {results_file}")
        
        return results_df

# ============================================================================
# ã€æ•°æ®åˆ†æå’Œå¯è§†åŒ–ã€‘
# ============================================================================

class ResultAnalyzer:
    """ç»“æœåˆ†æå™¨"""
    
    def __init__(self, results_df: pd.DataFrame, logger: logging.Logger):
        """
        åˆå§‹åŒ–ç»“æœåˆ†æå™¨
        """
        self.results_df = results_df
        self.logger = logger
        
        # ç»˜å›¾é…ç½®
        self.plot_methods = ['fft', 'stft', 'dct', 'wavelet', 'emd', 'eemd', 'vmd']
        self.colors = plt.cm.tab10(np.linspace(0, 1, len(self.plot_methods)))
        self.markers = ['o', 's', '^', 'D', 'v', '<', '>']
        self.window_ticks = [30, 60, 90, 120] # æ–°å¢ï¼šæ¨ªåæ ‡åˆ»åº¦
    
    # ****************************************************************************
    # ã€ä¿®æ”¹ï¼šåŸ _plot_metric_comparison æ‹†åˆ†ä¸ºå•å›¾ç»˜åˆ¶ã€‘
    # ****************************************************************************
    def _plot_single_metric_comparison(self, metric_key: str, ylabel: str, title: str):
        """ç»˜åˆ¶å•ä¸ªæŒ‡æ ‡çš„å¯¹æ¯”å›¾ (çª—å£é•¿åº¦) - 4å¼ å•å›¾ä¹‹ä¸€"""
        fig, ax = plt.subplots(figsize=(12, 8))
        
        for method, color, marker in zip(self.plot_methods, self.colors, self.markers):
            method_data = self.results_df[self.results_df['method'] == method].sort_values('window_s')
            
            if len(method_data) > 0:
                ax.plot(method_data['window_s'], method_data[metric_key],
                        marker=marker, markersize=10, linewidth=2.5, 
                        color=color, label=method.upper(), alpha=0.8)
        
        ax.set_xlabel('Window Length (s)', fontweight='bold')
        ax.set_ylabel(ylabel, fontweight='bold')
        ax.set_title(title, fontweight='bold')
        ax.set_xticks(self.window_ticks)  # <<< ä¿®æ”¹ç‚¹
        ax.legend(loc='best', framealpha=0.9)
        ax.grid(True, alpha=0.3, linestyle='--')
        
        plt.tight_layout()
        
        filename = f"{metric_key}_comparison_single.png"
        filepath = os.path.join(FIGURES_DIR, filename)
        plt.savefig(filepath, dpi=300, bbox_inches='tight')
        plt.close()
        self.logger.info(f" Â âœ… å•å›¾: {filename}")

    # ****************************************************************************
    # ã€æ–°å¢ï¼šçª—å£é•¿åº¦ 2x2 ç»„åˆå›¾ã€‘
    # ****************************************************************************
    def _plot_combined_window_comparison(self):
        """ç»˜åˆ¶ 2x2 çª—å£é•¿åº¦å¯¹æ¯”ç»„åˆå›¾ - 1å¼ ç»„åˆå›¾"""
        self.logger.info(" Â ğŸš€ ç»˜åˆ¶ 2x2 çª—å£é•¿åº¦ç»„åˆå›¾")
        
        metrics = [
            ('loa_range', 'LoA Range (bpm)', '(a) LoA Range'),
            ('mean_difference', 'Mean Difference (bpm)', '(b) Mean Difference'),
            ('mae', 'MAE (bpm)', '(c) MAE'),
            ('rmse', 'RMSE (bpm)', '(d) RMSE')
        ]
        
        fig, axes = plt.subplots(2, 2, figsize=(20, 11))
        axes = axes.flatten()
        
        for i, (metric_key, ylabel, sub_title) in enumerate(metrics):
            ax = axes[i]
            
            for method, color, marker in zip(self.plot_methods, self.colors, self.markers):
                method_data = self.results_df[self.results_df['method'] == method].sort_values('window_s')
                
                if len(method_data) > 0:
                    ax.plot(method_data['window_s'], method_data[metric_key],
                            marker=marker, markersize=10, linewidth=2.5, 
                            color=color, label=method.upper(), alpha=0.8)
            
            ax.set_xlabel('Window Length (s)', fontweight='bold')
            ax.set_ylabel(ylabel, fontweight='bold')
            ax.set_title(sub_title, fontweight='bold')
            ax.set_xticks(self.window_ticks) # <<< ä¿®æ”¹ç‚¹
            ax.grid(True, alpha=0.3, linestyle='--')
        
        # ç»Ÿä¸€å›¾ä¾‹æ”¾åœ¨å³ä¸‹è§’
        handles, labels = axes[0].get_legend_handles_labels()
        # è°ƒæ•´å›¾ä¾‹å­—ä½“å¤§å°ï¼ˆè¿™é‡Œä½¿ç”¨ FONT_SIZE_LEGENDï¼‰
        fig.legend(handles, labels, loc='lower center', bbox_to_anchor=(0.5, 0.01), 
                   ncol=len(self.plot_methods), framealpha=0.9) 

        # è°ƒæ•´å­å›¾é—´è·
        plt.subplots_adjust(hspace=0.35, wspace=0.2, bottom=0.15)
        
        # æ·»åŠ æ€»æ ‡é¢˜ (ä½¿ç”¨ FONT_SIZE_TITLE+2 ç¡®ä¿æ¯”å­å›¾æ ‡é¢˜å¤§)
        fig.suptitle('Performance Metrics vs Window Length Comparison', fontsize=FONT_SIZE_TITLE + 2, fontweight='bold')
        
        filename = "combined_window_comparison.png"
        filepath = os.path.join(FIGURES_DIR, filename)
        plt.savefig(filepath, dpi=300, bbox_inches='tight')
        plt.close()
        self.logger.info(f" Â âœ… ç»„åˆå›¾: {filename}")


    # ****************************************************************************
    # ã€ä¿®æ”¹ï¼š generate_comparison_plots åŒ…å« 4 å¼ å•å›¾ + 1 å¼ ç»„åˆå›¾ã€‘
    # ****************************************************************************
    def generate_comparison_plots(self):
        """ç”Ÿæˆ4å¼ å•å›¾å’Œ1å¼ ç»„åˆå¯¹æ¯”å›¾ (çª—å£é•¿åº¦)"""
        self.logger.info("\n" + "=" * 70)
        self.logger.info("ğŸ“Š ç”Ÿæˆçª—å£é•¿åº¦å¯¹æ¯”å›¾ (4å¼ å•å›¾ + 1å¼ ç»„åˆå›¾)")
        self.logger.info("=" * 70)
        
        metrics = [
            ('loa_range', 'LoA Range (bpm)', 'Limits of Agreement Range vs Window Length'),
            ('mean_difference', 'Mean Difference (bpm)', 'Mean Difference vs Window Length'),
            ('mae', 'MAE (bpm)', 'Mean Absolute Error vs Window Length'),
            ('rmse', 'RMSE (bpm)', 'Root Mean Square Error vs Window Length')
        ]
        
        # 1. ç»˜åˆ¶ 4 å¼ å•å›¾
        for metric_key, ylabel, title in metrics:
            self._plot_single_metric_comparison(metric_key, ylabel, title)
        
        # 2. ç»˜åˆ¶ 1 å¼ ç»„åˆå›¾
        self._plot_combined_window_comparison()
    
    def generate_summary_table(self):
        """ç”Ÿæˆæ±‡æ€»è¡¨"""
        self.logger.info("\n" + "=" * 70)
        self.logger.info("ğŸ“‹ ç”Ÿæˆæ±‡æ€»è¡¨")
        self.logger.info("=" * 70)
        
        # Excelæ–‡ä»¶ï¼šæ¯ä¸ªæ–¹æ³•ä¸€ä¸ªsheet
        excel_file = os.path.join(TABLES_DIR, 'summary_tables.xlsx')
        
        with pd.ExcelWriter(excel_file, engine='openpyxl') as writer:
            for method in self.plot_methods:
                method_data = self.results_df[self.results_df['method'] == method].sort_values('window_s')
                
                if len(method_data) > 0:
                    # é€‰æ‹©è¦æ˜¾ç¤ºçš„åˆ—
                    table = method_data[[
                        'window_s', 'loa_range', 'mean_difference', 'mae', 'rmse',
                        'correlation', 'mean_computation_time_per_segment_ms'
                    ]].copy()
                    
                    # é‡å‘½ååˆ—
                    table.columns = [
                        'Window (s)', 'LoA Range', 'Mean Diff', 'MAE', 'RMSE',
                        'Correlation', 'Time (ms)'
                    ]
                    
                    # æ ¼å¼åŒ–æ•°å€¼
                    for col in ['LoA Range', 'Mean Diff', 'MAE', 'RMSE']:
                        table[col] = table[col].round(2)
                    table['Correlation'] = table['Correlation'].round(4)
                    table['Time (ms)'] = table['Time (ms)'].round(2)
                    
                    # å†™å…¥Excel
                    table.to_excel(writer, sheet_name=method.upper(), index=False)
                    
                    self.logger.info(f" Â âœ… {method.upper()}: {len(table)} rows")
        
        self.logger.info(f"\nâœ… Excelè¡¨æ ¼å·²ä¿å­˜è‡³: {excel_file}")
        
        # CSVæ–‡ä»¶ï¼šæ‰€æœ‰æ•°æ®åœ¨ä¸€ä¸ªæ–‡ä»¶
        csv_file = os.path.join(TABLES_DIR, 'summary_all_methods.csv')
        summary_csv = self.results_df[[
            'method', 'window_s', 'loa_range', 'mean_difference', 'mae', 'rmse',
            'correlation', 'mean_computation_time_per_segment_ms', 'n_samples'
        ]].sort_values(['method', 'window_s'])
        
        summary_csv.to_csv(csv_file, index=False, encoding='utf-8-sig')
        self.logger.info(f"âœ… CSVè¡¨æ ¼å·²ä¿å­˜è‡³: {csv_file}")
        
        # Markdownè¡¨æ ¼ï¼šç”¨äºæ–‡æ¡£
        self._generate_markdown_tables()
    
    def _generate_markdown_tables(self):
        """ç”ŸæˆMarkdownæ ¼å¼çš„è¡¨æ ¼"""
        md_file = os.path.join(TABLES_DIR, 'summary_tables.md')
        
        with open(md_file, 'w', encoding='utf-8') as f:
            f.write("# æ‰¹é‡å®éªŒç»“æœæ±‡æ€»\n\n")
            
            for method in self.plot_methods:
                method_data = self.results_df[self.results_df['method'] == method].sort_values('window_s')
                
                if len(method_data) > 0:
                    f.write(f"## {method.upper()}\n\n")
                    f.write("| Window (s) | LoA Range | Mean Diff | MAE | RMSE | Time (ms) |\n")
                    f.write("|------------|-----------|-----------|-----|------|----------|\n")
                    
                    for _, row in method_data.iterrows():
                        f.write(f"| {row['window_s']} | {row['loa_range']:.2f} | "
                                f"{row['mean_difference']:.2f} | {row['mae']:.2f} | "
                                f"{row['rmse']:.2f} | {row['mean_computation_time_per_segment_ms']:.2f} |\n")
                    
                    f.write("\n")
        
        self.logger.info(f"âœ… Markdownè¡¨æ ¼å·²ä¿å­˜è‡³: {md_file}")
    
    def print_summary_statistics(self):
        """æ‰“å°æ±‡æ€»ç»Ÿè®¡"""
        self.logger.info("\n" + "=" * 70)
        self.logger.info("ğŸ“ˆ æ±‡æ€»ç»Ÿè®¡")
        self.logger.info("=" * 70)
        
        for method in self.plot_methods:
            method_data = self.results_df[self.results_df['method'] == method]
            
            if len(method_data) > 0:
                self.logger.info(f"\n{method.upper()}:")
                self.logger.info(f" Â å®éªŒæ•°: {len(method_data)}")
                self.logger.info(f" Â MAEèŒƒå›´: {method_data['mae'].min():.2f} - {method_data['mae'].max():.2f}")
                self.logger.info(f" Â RMSEèŒƒå›´: {method_data['rmse'].min():.2f} - {method_data['rmse'].max():.2f}")
                self.logger.info(f" Â æœ€ä½³çª—å£(MAE): {method_data.loc[method_data['mae'].idxmin(), 'window_s']:.0f}s")
                self.logger.info(f" Â å¹³å‡è®¡ç®—æ—¶é—´: {method_data['mean_computation_time_per_segment_ms'].mean():.2f}ms")
    
    # ****************************************************************************
    # ã€ä¿®æ”¹ï¼šåŸ _plot_distance_comparison æ‹†åˆ†ä¸ºå•å›¾ç»˜åˆ¶ã€‘
    # ****************************************************************************
    def _plot_single_distance_comparison(self, dist_df, window_s, metric_key: str, ylabel: str, title: str):
        """ç»˜åˆ¶å•ä¸ªæŒ‡æ ‡çš„è·ç¦»å¯¹æ¯”å›¾ (4å¼ å•å›¾ä¹‹ä¸€)"""
        fig, ax = plt.subplots(figsize=(10, 7))
        
        for method, color, marker in zip(self.plot_methods, self.colors, self.markers):
            method_data = dist_df[dist_df['method'] == method].sort_values('distance')
            
            if len(method_data) > 0:
                ax.plot(method_data['distance'], method_data[metric_key],
                        marker=marker, markersize=10, linewidth=2.5,
                        color=color, label=method.upper(), alpha=0.8)
        
        ax.set_xlabel('Distance (cm)', fontweight='bold')
        ax.set_ylabel(ylabel, fontweight='bold')
        ax.set_title(title, fontweight='bold')
        ax.set_xticks([40, 50, 60])
        ax.legend(loc='best', framealpha=0.9)
        ax.grid(True, alpha=0.3, linestyle='--')
        
        plt.tight_layout()
        
        filename = f"distance_{metric_key}_w{window_s}s_single.png"
        filepath = os.path.join(FIGURES_DIR, filename)
        plt.savefig(filepath, dpi=300, bbox_inches='tight')
        plt.close()
        self.logger.info(f" Â âœ… å•å›¾: {filename}")

    # ****************************************************************************
    # ã€æ–°å¢ï¼šè·ç¦» 2x2 ç»„åˆå›¾ã€‘
    # ****************************************************************************
    def _plot_combined_distance_comparison(self, dist_df, window_s):
        """ç»˜åˆ¶ 2x2 è·ç¦»å¯¹æ¯”ç»„åˆå›¾ - 1å¼ ç»„åˆå›¾"""
        self.logger.info(" Â ğŸš€ ç»˜åˆ¶ 2x2 è·ç¦»åˆ†æç»„åˆå›¾")
        
        metrics = [
            ('loa_range', 'LoA Range (bpm)', '(a) LoA Range'),
            ('mean_difference', 'Mean Difference (bpm)', '(b) Mean Difference'),
            ('mae', 'MAE (bpm)', '(c) MAE'),
            ('rmse', 'RMSE (bpm)', '(d) RMSE')
        ]
        
        fig, axes = plt.subplots(2, 2, figsize=(20, 11))
        axes = axes.flatten()
        
        for i, (metric_key, ylabel, sub_title) in enumerate(metrics):
            ax = axes[i]
            
            for method, color, marker in zip(self.plot_methods, self.colors, self.markers):
                method_data = dist_df[dist_df['method'] == method].sort_values('distance')
                
                if len(method_data) > 0:
                    ax.plot(method_data['distance'], method_data[metric_key],
                            marker=marker, markersize=10, linewidth=2.5,
                            color=color, label=method.upper(), alpha=0.8)
            
            ax.set_xlabel('Distance (cm)', fontweight='bold')
            ax.set_ylabel(ylabel, fontweight='bold')
            ax.set_title(sub_title, fontweight='bold')
            ax.set_xticks([40, 50, 60])
            ax.grid(True, alpha=0.3, linestyle='--')

        # ç»Ÿä¸€å›¾ä¾‹æ”¾åœ¨å³ä¸‹è§’
        handles, labels = axes[0].get_legend_handles_labels()
        # è°ƒæ•´å›¾ä¾‹å­—ä½“å¤§å°ï¼ˆè¿™é‡Œä½¿ç”¨ FONT_SIZE_LEGENDï¼‰
        fig.legend(handles, labels, loc='lower center', bbox_to_anchor=(0.5, 0.01), 
                   ncol=len(self.plot_methods), framealpha=0.9)

        # è°ƒæ•´å­å›¾é—´è·
        plt.subplots_adjust(hspace=0.35, wspace=0.2, bottom=0.15)
        
        # æ·»åŠ æ€»æ ‡é¢˜
        fig.suptitle(f'Performance Metrics vs Distance Comparison (Window={window_s}s)', fontsize=FONT_SIZE_TITLE + 2, fontweight='bold')
        
        filename = f"combined_distance_analysis_w{window_s}s.png"
        filepath = os.path.join(FIGURES_DIR, filename)
        plt.savefig(filepath, dpi=300, bbox_inches='tight')
        plt.close()
        self.logger.info(f" Â âœ… ç»„åˆå›¾: {filename}")
    
    # ****************************************************************************
    # ã€ä¿®æ”¹ï¼š generate_distance_analysis_plots åŒ…å« 4 å¼ å•å›¾ + 1 å¼ ç»„åˆå›¾ã€‘
    # ****************************************************************************
    def generate_distance_analysis_plots(self, window_s=120):
        """
        ç”Ÿæˆè·ç¦»åˆ†æå›¾ï¼ˆ4å¼ å•å›¾ + 1å¼ ç»„åˆå›¾ï¼Œå›ºå®šçª—å£é•¿åº¦ï¼‰
        """
        self.logger.info("\n" + "=" * 70)
        self.logger.info(f"ğŸ“Š ç”Ÿæˆè·ç¦»åˆ†æå›¾ï¼ˆçª—å£é•¿åº¦={window_s}s, 4å¼ å•å›¾ + 1å¼ ç»„åˆå›¾ï¼‰")
        self.logger.info("=" * 70)
        
        # 1. ç­›é€‰æŒ‡å®šçª—å£é•¿åº¦çš„å®éªŒ
        target_experiments = self.results_df[self.results_df['window_s'] == window_s]
        
        if len(target_experiments) == 0:
            self.logger.warning(f"âš ï¸ æ²¡æœ‰æ‰¾åˆ°çª—å£é•¿åº¦ä¸º{window_s}sçš„å®éªŒæ•°æ®")
            return
        
        self.logger.info(f"æ‰¾åˆ° {len(target_experiments)} ä¸ªæ–¹æ³•çš„å®éªŒæ•°æ®")
        
        # 2. å¯¹æ¯ä¸ªå®éªŒï¼Œè¯»å–CSVå¹¶æŒ‰è·ç¦»ç»Ÿè®¡
        distance_data = {
            'method': [],
            'distance': [],
            'mae': [],
            'rmse': [],
            'loa_range': [],
            'mean_difference': []
        }
        
        for _, exp in target_experiments.iterrows():
            method = exp['method']
            exp_key = self._get_experiment_key(method, window_s)
            csv_path = os.path.join(CACHE_DIR, exp_key, 'heart_rate_results.csv')
            
            if os.path.exists(csv_path):
                try:
                    df = pd.read_csv(csv_path)
                    
                    # æŒ‰è·ç¦»åˆ†ç»„è®¡ç®—æŒ‡æ ‡
                    for distance in [40, 50, 60]:
                        dist_df = df[df['distance'] == distance].copy()
                        
                        if len(dist_df) == 0:
                            self.logger.warning(f" Â âš ï¸ {method.upper()} - {distance}cm: æ— æ•°æ®")
                            continue
                        
                        # è®¡ç®—è¯¯å·®
                        dist_df['error'] = dist_df['radar_hr_bpm'] - dist_df['neulog_hr_count']
                        
                        # å»é™¤NaNå€¼
                        valid_errors = dist_df['error'].dropna()
                        
                        if len(valid_errors) == 0:
                            self.logger.warning(f" Â âš ï¸ {method.upper()} - {distance}cm: æ— æœ‰æ•ˆæ•°æ®")
                            continue
                        
                        # è®¡ç®—MAE, RMSE
                        mae = valid_errors.abs().mean()
                        rmse = np.sqrt((valid_errors**2).mean())
                        
                        # è®¡ç®—Bland-AltmanæŒ‡æ ‡
                        mean_diff = valid_errors.mean()
                        std_diff = valid_errors.std()
                        loa_range = 1.96 * 2 * std_diff
                        
                        # ä¿å­˜æ•°æ®
                        distance_data['method'].append(method)
                        distance_data['distance'].append(distance)
                        distance_data['mae'].append(mae)
                        distance_data['rmse'].append(rmse)
                        distance_data['loa_range'].append(loa_range)
                        distance_data['mean_difference'].append(mean_diff)
                        
                        self.logger.info(f" Â âœ… {method.upper()} - {distance}cm: MAE={mae:.2f}, RMSE={rmse:.2f}")
                        
                except Exception as e:
                    self.logger.error(f" Â âŒ è¯»å– {exp_key} æ•°æ®å¤±è´¥: {e}")
            else:
                self.logger.warning(f" Â âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨: {csv_path}")
        
        # 3. è½¬æ¢ä¸ºDataFrame
        if len(distance_data['method']) == 0:
            self.logger.error("âŒ æ²¡æœ‰è¶³å¤Ÿçš„æ•°æ®ç”Ÿæˆè·ç¦»åˆ†æå›¾")
            return
        
        dist_df = pd.DataFrame(distance_data)
        
        # ä¿å­˜è·ç¦»åˆ†ææ•°æ®
        distance_csv = os.path.join(TABLES_DIR, f'distance_analysis_w{window_s}s.csv')
        dist_df.to_csv(distance_csv, index=False, encoding='utf-8-sig')
        self.logger.info(f"\nâœ… è·ç¦»åˆ†ææ•°æ®å·²ä¿å­˜: {distance_csv}")
        
        metrics = [
            ('mae', 'MAE (bpm)', f'MAE vs Distance (Window={window_s}s)'),
            ('rmse', 'RMSE (bpm)', f'RMSE vs Distance (Window={window_s}s)'),
            ('loa_range', 'LoA Range (bpm)', f'LoA Range vs Distance (Window={window_s}s)'),
            ('mean_difference', 'Mean Difference (bpm)', f'Mean Difference vs Distance (Window={window_s}s)')
        ]
        
        # 4. ç»˜åˆ¶ 4 å¼ å•å›¾
        for metric_key, ylabel, title in metrics:
            self._plot_single_distance_comparison(dist_df, window_s, metric_key, ylabel, title)

        # 5. ç»˜åˆ¶ 1 å¼ ç»„åˆå›¾
        self._plot_combined_distance_comparison(dist_df, window_s)
        
        self.logger.info(f"âœ… è·ç¦»åˆ†æå›¾ç”Ÿæˆå®Œæˆï¼ˆçª—å£={window_s}sï¼Œæ€»å…± 5 å¼ å›¾ï¼‰")
    
    # ç”±äº run_experiment è¢« mockï¼Œè¿™é‡Œéœ€è¦æ·»åŠ  _get_experiment_key æ¥æ”¯æŒ distance analysis
    def _get_experiment_key(self, method: str, window: int) -> str:
        """ç”Ÿæˆå®éªŒçš„å”¯ä¸€æ ‡è¯†"""
        return f"{method}_{window}s"


# ============================================================================
# ã€ä¸»å‡½æ•°ã€‘
# ============================================================================

def main():
    """ä¸»å‡½æ•°"""
    # è®¾ç½®æ—¥å¿—
    logger = setup_logging()
    
    logger.info("=" * 70)
    logger.info("ğŸš€ æ‰¹é‡å®éªŒç³»ç»Ÿå¯åŠ¨")
    logger.info("=" * 70)
    logger.info(f"æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info(f"å®éªŒé…ç½®: {sum(len(v) for v in EXPERIMENT_MATRIX.values())} ä¸ªå®éªŒ")
    
    # è¿è¡Œæ‰¹é‡å®éªŒ
    runner = BatchExperimentRunner(EXPERIMENT_MATRIX, logger)
    results_df = runner.run_all_experiments()
    
    # åˆ†æç»“æœ
    analyzer = ResultAnalyzer(results_df, logger)
    
    # ç”Ÿæˆçª—å£å¯¹æ¯”å›¾ (4 å¼ å•å›¾ + 1 å¼ ç»„åˆå›¾)
    analyzer.generate_comparison_plots()
    
    # ç”Ÿæˆæ±‡æ€»è¡¨
    analyzer.generate_summary_table()
    analyzer.print_summary_statistics()
    
    # ç”Ÿæˆè·ç¦»åˆ†æå›¾ï¼ˆå›ºå®šçª—å£é•¿åº¦120s, 4 å¼ å•å›¾ + 1 å¼ ç»„åˆå›¾ï¼‰
    logger.info("\n" + "=" * 70)
    logger.info("ğŸ“Š è·ç¦»å½±å“åˆ†æ")
    logger.info("=" * 70)
    analyzer.generate_distance_analysis_plots(window_s=120)
    
    # å®Œæˆ
    logger.info("\n" + "=" * 70)
    logger.info("âœ… æ‰¹é‡å®éªŒå®Œæˆï¼")
    logger.info("=" * 70)
    logger.info(f"\nè¾“å‡ºç›®å½•: {BATCH_OUTPUT_DIR}")
    logger.info(f" Â - å›¾è¡¨: {FIGURES_DIR}")
    logger.info(f" Â  Â * çª—å£å¯¹æ¯”å›¾ï¼ˆ4å¼ å•å›¾ + 1å¼ ç»„åˆå›¾ï¼‰")
    logger.info(f" Â  Â * è·ç¦»åˆ†æå›¾ï¼ˆ4å¼ å•å›¾ + 1å¼ ç»„åˆå›¾ï¼‰")
    logger.info(f" Â - è¡¨æ ¼: {TABLES_DIR}")
    logger.info(f" Â - ç¼“å­˜: {CACHE_DIR}")
    logger.info("=" * 70)
    logger.info("æ€»å›¾è¡¨æ•°ï¼š10 å¼ ")


if __name__ == "__main__":
    main()