"""
éšæœºæ£®æ— + CFAR Range Bin åˆ†ç±»å™¨ + STFT å¿ƒç‡ä¼°è®¡
Leave-One-File-Out äº¤å‰éªŒè¯ (K=12)
ä¸‰æ–¹å¯¹æ¯”ï¼šManual, RF, CFAR
"""

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import confusion_matrix
import warnings
warnings.filterwarnings('ignore')
import joblib
import json
from datetime import datetime


# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from FeatureExtract_1126_v7 import extract_features_from_all_files
from stft_estimator import STFTHeartRateEstimator
from evaluation_stft import STFTDualPathEvaluator
from cfar_detector import CAFARDetector  # ğŸ”¥ æ–°å¢

# ============================================================================
# ã€é…ç½®åŒºã€‘
# ============================================================================
print("=" * 70)
print("ğŸš€ éšæœºæ£®æ— + CFAR Range Bin é¢„æµ‹ + STFT å¿ƒç‡ä¼°è®¡ç³»ç»Ÿ")
print("   Leave-One-File-Out äº¤å‰éªŒè¯ (K=12)")
print("   ä¸‰æ–¹å¯¹æ¯”ï¼šManual, RF, CFAR")
print("=" * 70)

# --- æ–‡ä»¶é…ç½® ---
FILE_CONFIGS = [
    # 40cm - RB Index 6
    {'path': r"D:\MSc\Dissertation\Data\250902\test2\RangeFFT\RangeBin6_RX1\test2_NeuLogRadar_aligned_trimmed.mat", 'distance': 40, 'rb_index': 6, 'rx_index_example': 1},
    {'path': r"D:\MSc\Dissertation\Data\250902\test3\RangeFFT\RangeBin6_RX1\test3_NeuLogRadar_aligned_trimmed.mat", 'distance': 40, 'rb_index': 6, 'rx_index_example': 1},
    {'path': r"D:\MSc\Dissertation\Data\250902\test4\RangeFFT\RangeBin6_RX1\test4_NeuLogRadar_aligned_trimmed.mat", 'distance': 40, 'rb_index': 6, 'rx_index_example': 1},
    {'path': r"D:\MSc\Dissertation\Data\250902\test5\RangeFFT\RangeBin6_RX1\test5_NeuLogRadar_aligned_trimmed.mat", 'distance': 40, 'rb_index': 6, 'rx_index_example': 1},
    
    # 50cm - RB Index 7
    {'path': r"D:\MSc\Dissertation\Data\250826\test8\RangeFFT\RangeBin7_RX1\test8_NeuLogRadar_aligned_trimmed.mat", 'distance': 50, 'rb_index': 7, 'rx_index_example': 1},
    {'path': r"D:\MSc\Dissertation\Data\250826\test9\RangeFFT\RangeBin7_RX1\test9_NeuLogRadar_aligned_trimmed.mat", 'distance': 50, 'rb_index': 7, 'rx_index_example': 1},
    {'path': r"D:\MSc\Dissertation\Data\250925\test1\RangeFFT\RangeBin7_RX2\test1_NeuLogRadar_aligned_trimmed.mat", 'distance': 50, 'rb_index': 7, 'rx_index_example': 2},
    {'path': r"D:\MSc\Dissertation\Data\250925\test2\RangeFFT\RangeBin7_RX2\test2_NeuLogRadar_aligned_trimmed.mat", 'distance': 50, 'rb_index': 7, 'rx_index_example': 2},

    # 60cm - RB Index 8
    {'path': r"D:\MSc\Dissertation\Data\250925\test3\RangeFFT\RangeBin8_RX4\test3_NeuLogRadar_aligned_trimmed.mat", 'distance': 60, 'rb_index': 8, 'rx_index_example': 4},
    {'path': r"D:\MSc\Dissertation\Data\250925\test4\RangeFFT\RangeBin8_RX1\test4_NeuLogRadar_aligned_trimmed.mat", 'distance': 60, 'rb_index': 8, 'rx_index_example': 1},
    {'path': r"D:\MSc\Dissertation\Data\251016\test1\RangeFFT\RangeBin8_RX1\test1_NeuLogRadar_aligned_trimmed.mat", 'distance': 60, 'rb_index': 8, 'rx_index_example': 1},
    {'path': r"D:\MSc\Dissertation\Data\251016\test2\RangeFFT\RangeBin8_RX2\test2_NeuLogRadar_aligned_trimmed.mat", 'distance': 60, 'rb_index': 8, 'rx_index_example': 2},
]

# --- è®­ç»ƒé…ç½® ---
TRAIN_WINDOW_DURATION_S = 30
TRAIN_STEP_DURATION_S = 30

# --- æµ‹è¯•é…ç½® ---
TEST_WINDOW_DURATION_S = 120
TEST_STEP_DURATION_S = 15

# --- STFTé…ç½® ---
STFT_NPERSEG = 512
STFT_NOVERLAP = 384
STFT_WINDOW = 'hann'
STFT_FREQ_RANGE = (0.8, 2.0)

# --- CFARé…ç½® ğŸ”¥ æ–°å¢ ---
CFAR_G_R = 1
CFAR_G_D = 1
CFAR_L_R = 5
CFAR_L_D = 5
CFAR_P_FA = 1e-3
CFAR_N_DOPPLER_FFT = 128
CFAR_MIN_RANGE_BIN = 1
CFAR_MAX_RANGE_BIN = 18

# --- å…¶ä»–é…ç½® ---
TARGET_RX_INDEX = 1
RANDOM_STATE = 42
N_JOBS = -1

# --- ç‰¹å¾åç§° ---
FEATURE_NAMES = [
    'amp_mean', 'amp_std', 'amp_p2p', 'amp_skewness', 'amp_kurtosis',
    'phase_diff_std', 'phase_diff_range',
    'amp_energy_resp', 'amp_energy_cardiac', 'amp_life_energy_ratio',
    'amp_freq_peak_pos', 'amp_purity_ratio', 'amp_energy_ratio_C_R', 'amp_snr_db',
    'phase_energy_ratio_C_R', 'phase_snr_db',
    'amp_hr_peak_purity'
]

# --- è¾“å‡ºç›®å½• ---
OUTPUT_DIR = './results_rf_cfar_stft'
if not os.path.exists(OUTPUT_DIR):
    os.makedirs(OUTPUT_DIR)
    print(f"âœ… å·²åˆ›å»ºè¾“å‡ºç›®å½•: {OUTPUT_DIR}")

# è®¾ç½®ä¸­æ–‡å­—ä½“æ”¯æŒ
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

# ============================================================================
# ã€è¾…åŠ©å‡½æ•°ã€‘
# ============================================================================

def build_samples_from_segments(segments_list, target_rx_index):
    """ä»ç‰‡æ®µåˆ—è¡¨æ„å»ºäºŒåˆ†ç±»æ ·æœ¬"""
    samples_data = []
    
    for seg_idx, segment in enumerate(segments_list):
        file_name = segment['original_file']
        segment_id = segment['segment_index']
        heart_count = segment['heart_count']
        range_one_hot = segment['range_one_hot'].squeeze()
        distance = segment['distance']
        
        true_rb_index = np.argmax(range_one_hot) + 1
        rx_idx_0based = target_rx_index - 1
        R = segment['amp_mean'].shape[0]
        
        for r in range(R):
            rb_1based = r + 1
            
            feature_vector = []
            for feat_name in FEATURE_NAMES:
                if feat_name in segment:
                    feat_value = segment[feat_name][r, rx_idx_0based]
                    feature_vector.append(feat_value)
                else:
                    feature_vector.append(0.0)
            
            label = 1 if range_one_hot[r] == 1 else 0
            
            samples_data.append({
                'features': feature_vector,
                'label': label,
                'file_name': file_name,
                'segment_id': segment_id,
                'rb_index': rb_1based,
                'distance': distance,
                'heart_count': heart_count,
                'true_rb_index': true_rb_index,
                'segment_global_id': seg_idx
            })
    
    df = pd.DataFrame(samples_data)
    X = np.array(df['features'].tolist())
    y = df['label'].values
    meta_df = df.drop(columns=['features', 'label'])
    
    return X, y, meta_df


def perform_stft_comparison(segment, pred_rb, true_rb, rx_index, stft_estimator):
    """
    å¯¹é¢„æµ‹é”™è¯¯çš„çª—å£æ‰§è¡ŒSTFTåŒè·¯å¯¹æ¯”
    
    å‚æ•°:
        segment: æµ‹è¯•ç‰‡æ®µå­—å…¸ï¼ˆåŒ…å«raw_phase_dataï¼‰
        pred_rb: é¢„æµ‹çš„Range Bin (1-based)
        true_rb: çœŸå®çš„Range Bin (1-based)
        rx_index: RXå¤©çº¿ç´¢å¼• (1-based)
        stft_estimator: STFTä¼°è®¡å™¨å®ä¾‹
    
    è¿”å›:
        STFTç»“æœå­—å…¸
    """
    # æå–åŸå§‹phaseæ•°æ®
    raw_phase = segment['raw_phase_data']  # (R, nVX, T)
    rx_idx = rx_index - 1
    
    # è·¯å¾„A: ä½¿ç”¨é¢„æµ‹çš„Range Bin
    phase_pred = raw_phase[pred_rb - 1, rx_idx, :]
    hr_pred_result = stft_estimator.estimate(phase_pred, STFT_FREQ_RANGE)
    hr_pred = hr_pred_result['heart_rate_bpm']
    
    # è·¯å¾„B: ä½¿ç”¨çœŸå®çš„Range Bin
    phase_true = raw_phase[true_rb - 1, rx_idx, :]
    hr_true_result = stft_estimator.estimate(phase_true, STFT_FREQ_RANGE)
    hr_true = hr_true_result['heart_rate_bpm']
    
    # NeuLogçœŸå®å¿ƒç‡
    heart_count = segment['heart_count']
    window_duration = TEST_WINDOW_DURATION_S
    neulog_hr = (heart_count / window_duration) * 60
    
    return {
        'neulog_hr_bpm': neulog_hr,
        'hr_from_pred_rb': hr_pred,
        'hr_from_true_rb': hr_true,
        'mae_pred': abs(hr_pred - neulog_hr) if not np.isnan(hr_pred) else np.nan,
        'mae_true': abs(hr_true - neulog_hr) if not np.isnan(hr_true) else np.nan
    }

# ğŸ”¥ æ–°å¢ï¼šCFARé¢„æµ‹å‡½æ•°
def perform_cfar_prediction(segment, cfar_detector, rx_index):
    """
    ä½¿ç”¨CFARæ£€æµ‹Range Bin
    
    å‚æ•°:
        segment: æµ‹è¯•ç‰‡æ®µå­—å…¸ï¼ˆåŒ…å«raw_phase_dataï¼‰
        cfar_detector: CFARæ£€æµ‹å™¨å®ä¾‹
        rx_index: RXå¤©çº¿ç´¢å¼• (1-basedï¼Œä½†CFARç”¨å…¨éƒ¨RX)
    
    è¿”å›:
        detected_rb: æ£€æµ‹åˆ°çš„Range Bin (1-based)
        confidence: ç½®ä¿¡åº¦
    """
    # æå–åŸå§‹phaseæ•°æ® (R, nVX, T)
    raw_phase = segment['raw_phase_data']
    
    # CFARéœ€è¦ (Range x RX x Time) æ ¼å¼ï¼Œå·²ç»ç¬¦åˆ
    cfar_result = cfar_detector.detect(raw_phase)
    
    detected_rb = cfar_result['detected_range_bin']
    confidence = cfar_result['confidence']
    
    return detected_rb, confidence


# ============================================================================
# ã€ä¸»ç¨‹åºï¼šLeave-One-File-Out äº¤å‰éªŒè¯ã€‘
# ============================================================================

print("\nã€é…ç½®ä¿¡æ¯ã€‘")
print(f"è®­ç»ƒçª—å£: {TRAIN_WINDOW_DURATION_S}s (æ­¥é•¿{TRAIN_STEP_DURATION_S}s, æ— é‡å )")
print(f"æµ‹è¯•çª—å£: {TEST_WINDOW_DURATION_S}s (æ­¥é•¿{TEST_STEP_DURATION_S}s, æœ‰é‡å )")
print(f"STFTå‚æ•°: nperseg={STFT_NPERSEG}, noverlap={STFT_NOVERLAP}")
print(f"æ€»æ–‡ä»¶æ•°: {len(FILE_CONFIGS)} (K={len(FILE_CONFIGS)})")

# åˆå§‹åŒ–STFTä¼°è®¡å™¨
stft_estimator = STFTHeartRateEstimator(
    fs=20.0,
    detrend=True,
    nperseg=STFT_NPERSEG,
    noverlap=STFT_NOVERLAP,
    window=STFT_WINDOW
)

# ğŸ”¥ åˆå§‹åŒ–CFARæ£€æµ‹å™¨
cfar_detector = CAFARDetector(
    G_R=CFAR_G_R,
    G_D=CFAR_G_D,
    L_R=CFAR_L_R,
    L_D=CFAR_L_D,
    P_fa=CFAR_P_FA,
    n_doppler_fft=CFAR_N_DOPPLER_FFT,
    min_range_bin=CFAR_MIN_RANGE_BIN,
    max_range_bin=CFAR_MAX_RANGE_BIN
)

# å­˜å‚¨æ‰€æœ‰foldçš„ç»“æœ
all_fold_summary = []
all_rf_predictions = []
all_cfar_predictions = []  # ğŸ”¥ æ–°å¢
all_feature_importances = []  # ğŸ”¥ æ–°å¢ï¼šå­˜å‚¨ç‰¹å¾é‡è¦æ€§

# å¼€å§‹K-Foldå¾ªç¯
K = len(FILE_CONFIGS)

for fold_idx in range(K):
    print("\n" + "=" * 70)
    print(f"Fold {fold_idx + 1}/{K}: æµ‹è¯•æ–‡ä»¶ {os.path.basename(FILE_CONFIGS[fold_idx]['path'])} ({FILE_CONFIGS[fold_idx]['distance']}cm)")
    print("=" * 70)
    
    # 1. åˆ’åˆ†è®­ç»ƒ/æµ‹è¯•æ–‡ä»¶
    test_file = FILE_CONFIGS[fold_idx]
    train_files = FILE_CONFIGS[:fold_idx] + FILE_CONFIGS[fold_idx+1:]
    
    # ========================================================================
    # ã€RFè®­ç»ƒã€‘
    # ========================================================================
    print("\nã€RF è®­ç»ƒé˜¶æ®µã€‘")
    train_segments = extract_features_from_all_files(
        train_files,
        window_duration_s=TRAIN_WINDOW_DURATION_S,
        step_duration_s=TRAIN_STEP_DURATION_S,
        keep_raw_phase=False
    )
    
    X_train, y_train, meta_train = build_samples_from_segments(train_segments, TARGET_RX_INDEX)
    
    print(f"RFè®­ç»ƒé›†: {len(train_files)}ä¸ªæ–‡ä»¶, {len(np.unique(meta_train['segment_global_id']))}ä¸ªçª—å£, {len(y_train)}ä¸ªæ ·æœ¬")
    
    # æ ‡å‡†åŒ– + è®­ç»ƒæ¨¡å‹
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    
    rf_model = RandomForestClassifier(
        n_estimators=200,
        max_depth=20,
        min_samples_split=5,
        min_samples_leaf=2,
        class_weight='balanced',
        random_state=RANDOM_STATE,
        n_jobs=N_JOBS
    )
    
    rf_model.fit(X_train_scaled, y_train)
    print(f"âœ… RFæ¨¡å‹è®­ç»ƒå®Œæˆ")

    # ğŸ”¥ æ–°å¢ï¼šä¿å­˜è¯¥foldçš„ç‰¹å¾é‡è¦æ€§
    feature_importance = rf_model.feature_importances_
    all_feature_importances.append(feature_importance)
    
    # ========================================================================
    # ã€æµ‹è¯•é˜¶æ®µï¼šRF + CFARã€‘
    # ========================================================================
    print("\nã€æµ‹è¯•é˜¶æ®µï¼šRF + CFARã€‘")
    test_segments = extract_features_from_all_files(
        [test_file],
        window_duration_s=TEST_WINDOW_DURATION_S,
        step_duration_s=TEST_STEP_DURATION_S,
        keep_raw_phase=True
    )
    
    num_test_windows = len(test_segments)
    print(f"æµ‹è¯•é›†: 1ä¸ªæ–‡ä»¶, {num_test_windows}ä¸ªçª—å£")
    
    # å¯¹æ¯ä¸ªæµ‹è¯•çª—å£è¿›è¡Œé¢„æµ‹
    fold_rf_predictions = []
    fold_cfar_predictions = []
    
    rf_correct_count = 0
    cfar_correct_count = 0
    
    print(f"\nğŸ“Š é€çª—å£é¢„æµ‹...")
    
    for window_idx, segment in enumerate(test_segments):
        
        true_rb = segment['rb_index_1based']
        
        # ====================================================================
        # RF é¢„æµ‹
        # ====================================================================
        X_test_window = []
        rx_idx = TARGET_RX_INDEX - 1
        R = segment['amp_mean'].shape[0]
        
        for r in range(R):
            feature_vector = []
            for feat_name in FEATURE_NAMES:
                if feat_name in segment:
                    feat_value = segment[feat_name][r, rx_idx]
                    feature_vector.append(feat_value)
                else:
                    feature_vector.append(0.0)
            X_test_window.append(feature_vector)
        
        X_test_window = np.array(X_test_window)
        X_test_scaled = scaler.transform(X_test_window)
        
        probas = rf_model.predict_proba(X_test_scaled)[:, 1]
        rf_pred_rb = np.argmax(probas) + 1
        rf_is_correct = (rf_pred_rb == true_rb)
        
        if rf_is_correct:
            rf_correct_count += 1
        
        # RFé¢„æµ‹è®°å½•
        rf_prediction_record = {
            'fold': fold_idx + 1,
            'file_name': segment['original_file'],
            'window_index': segment['segment_index'],
            'distance': segment['distance'],
            'true_rb': true_rb,
            'pred_rb': rf_pred_rb,
            'is_correct': rf_is_correct,
            'pred_proba': probas[rf_pred_rb - 1],
            'heart_count': segment['heart_count']
        }
        
        # å¦‚æœRFé¢„æµ‹é”™è¯¯ï¼Œæ‰§è¡ŒSTFT
        if not rf_is_correct:
            stft_result = perform_stft_comparison(
                segment, rf_pred_rb, true_rb, TARGET_RX_INDEX, stft_estimator
            )
            rf_prediction_record.update(stft_result)
            
            print(f"  RF âœ— çª—å£{segment['segment_index']}: é¢„æµ‹={rf_pred_rb}, çœŸå®={true_rb} | "
                  f"HR: {stft_result['hr_from_pred_rb']:.1f} vs {stft_result['hr_from_true_rb']:.1f} "
                  f"(NeuLog={stft_result['neulog_hr_bpm']:.1f})")
        
        fold_rf_predictions.append(rf_prediction_record)
        
        # ====================================================================
        # CFAR é¢„æµ‹ ğŸ”¥
        # ====================================================================
        cfar_pred_rb, cfar_confidence = perform_cfar_prediction(
            segment, cfar_detector, TARGET_RX_INDEX
        )
        cfar_is_correct = (cfar_pred_rb == true_rb)
        
        if cfar_is_correct:
            cfar_correct_count += 1
        
        # CFARé¢„æµ‹è®°å½•
        cfar_prediction_record = {
            'fold': fold_idx + 1,
            'file_name': segment['original_file'],
            'window_index': segment['segment_index'],
            'distance': segment['distance'],
            'true_rb': true_rb,
            'pred_rb': cfar_pred_rb,
            'is_correct': cfar_is_correct,
            'confidence': cfar_confidence,
            'heart_count': segment['heart_count']
        }
        
        # å¦‚æœCFARé¢„æµ‹é”™è¯¯ï¼Œæ‰§è¡ŒSTFT
        if not cfar_is_correct:
            stft_result = perform_stft_comparison(
                segment, cfar_pred_rb, true_rb, TARGET_RX_INDEX, stft_estimator
            )
            cfar_prediction_record.update(stft_result)
            
            print(f"  CFAR âœ— çª—å£{segment['segment_index']}: é¢„æµ‹={cfar_pred_rb}, çœŸå®={true_rb} | "
                  f"HR: {stft_result['hr_from_pred_rb']:.1f} vs {stft_result['hr_from_true_rb']:.1f} "
                  f"(NeuLog={stft_result['neulog_hr_bpm']:.1f})")
        
        fold_cfar_predictions.append(cfar_prediction_record)
    
    # ========================================================================
    # Foldç»“æœæ±‡æ€»
    # ========================================================================
    rf_accuracy = rf_correct_count / num_test_windows if num_test_windows > 0 else 0.0
    cfar_accuracy = cfar_correct_count / num_test_windows if num_test_windows > 0 else 0.0
    
    fold_summary = {
        'fold': fold_idx + 1,
        'test_file': os.path.basename(test_file['path']),
        'distance': test_file['distance'],
        'n_test_windows': num_test_windows,
        'rf_correct': rf_correct_count,
        'rf_wrong': num_test_windows - rf_correct_count,
        'rf_accuracy': rf_accuracy,
        'cfar_correct': cfar_correct_count,
        'cfar_wrong': num_test_windows - cfar_correct_count,
        'cfar_accuracy': cfar_accuracy
    }
    
    all_fold_summary.append(fold_summary)
    all_rf_predictions.extend(fold_rf_predictions)
    all_cfar_predictions.extend(fold_cfar_predictions)
    
    print(f"\nâœ… Fold {fold_idx + 1} å®Œæˆ:")
    print(f"   RF:   æ­£ç¡® {rf_correct_count}/{num_test_windows} ({rf_accuracy*100:.1f}%)")
    print(f"   CFAR: æ­£ç¡® {cfar_correct_count}/{num_test_windows} ({cfar_accuracy*100:.1f}%)")
    
    # ä¿å­˜æ¨¡å‹
    model_path = os.path.join(OUTPUT_DIR, f'rf_model_fold{fold_idx+1}.pkl')
    scaler_path = os.path.join(OUTPUT_DIR, f'scaler_fold{fold_idx+1}.pkl')
    joblib.dump(rf_model, model_path)
    joblib.dump(scaler, scaler_path)

# ============================================================================
# ã€æ±‡æ€»æ‰€æœ‰Foldç»“æœã€‘
# ============================================================================

print("\n" + "=" * 70)
print("âœ… æ‰€æœ‰Foldå®Œæˆï¼")
print("=" * 70)

# è½¬æ¢ä¸ºDataFrame
fold_summary_df = pd.DataFrame(all_fold_summary)
rf_predictions_df = pd.DataFrame(all_rf_predictions)
cfar_predictions_df = pd.DataFrame(all_cfar_predictions)

# ä¿å­˜åŸå§‹ç»“æœ
fold_summary_df.to_csv(os.path.join(OUTPUT_DIR, 'fold_summary.csv'), index=False)
rf_predictions_df.to_csv(os.path.join(OUTPUT_DIR, 'rf_predictions.csv'), index=False)
cfar_predictions_df.to_csv(os.path.join(OUTPUT_DIR, 'cfar_predictions.csv'), index=False)

# ============================================================================
# ã€ç»Ÿè®¡ç»“æœã€‘
# ============================================================================

print("\nã€Range Bin é¢„æµ‹å‡†ç¡®ç‡æ±‡æ€»ã€‘")

# RFå‡†ç¡®ç‡
print("\n=== éšæœºæ£®æ— (RF) ===")
print("\nå„Foldå‡†ç¡®ç‡:")
for _, row in fold_summary_df.iterrows():
    print(f"  Fold {int(row['fold'])} ({int(row['distance'])}cm): "
          f"{int(row['rf_correct'])}/{int(row['n_test_windows'])} = {row['rf_accuracy']*100:.1f}%")

rf_total_windows = fold_summary_df['n_test_windows'].sum()
rf_total_correct = fold_summary_df['rf_correct'].sum()
rf_overall_accuracy = rf_total_correct / rf_total_windows

print(f"\nRFå…¨å±€ç»Ÿè®¡:")
print(f"  æ€»çª—å£æ•°: {rf_total_windows}")
print(f"  é¢„æµ‹æ­£ç¡®: {rf_total_correct} ({rf_overall_accuracy*100:.1f}%)")
print(f"  é¢„æµ‹é”™è¯¯: {rf_total_windows - rf_total_correct} ({(1-rf_overall_accuracy)*100:.1f}%)")

print(f"\nRFæŒ‰è·ç¦»ç»Ÿè®¡:")
for dist in sorted(fold_summary_df['distance'].unique()):
    dist_data = fold_summary_df[fold_summary_df['distance'] == dist]
    dist_correct = dist_data['rf_correct'].sum()
    dist_total = dist_data['n_test_windows'].sum()
    dist_acc = dist_correct / dist_total
    print(f"  {int(dist)}cm: {int(dist_correct)}/{int(dist_total)} = {dist_acc*100:.1f}%")

# CFARå‡†ç¡®ç‡
print("\n=== CA-CFAR ===")
print("\nå„Foldå‡†ç¡®ç‡:")
for _, row in fold_summary_df.iterrows():
    print(f"  Fold {int(row['fold'])} ({int(row['distance'])}cm): "
          f"{int(row['cfar_correct'])}/{int(row['n_test_windows'])} = {row['cfar_accuracy']*100:.1f}%")

cfar_total_correct = fold_summary_df['cfar_correct'].sum()
cfar_overall_accuracy = cfar_total_correct / rf_total_windows

print(f"\nCFARå…¨å±€ç»Ÿè®¡:")
print(f"  æ€»çª—å£æ•°: {rf_total_windows}")
print(f"  é¢„æµ‹æ­£ç¡®: {cfar_total_correct} ({cfar_overall_accuracy*100:.1f}%)")
print(f"  é¢„æµ‹é”™è¯¯: {rf_total_windows - cfar_total_correct} ({(1-cfar_overall_accuracy)*100:.1f}%)")

print(f"\nCFARæŒ‰è·ç¦»ç»Ÿè®¡:")
for dist in sorted(fold_summary_df['distance'].unique()):
    dist_data = fold_summary_df[fold_summary_df['distance'] == dist]
    dist_correct = dist_data['cfar_correct'].sum()
    dist_total = dist_data['n_test_windows'].sum()
    dist_acc = dist_correct / dist_total
    print(f"  {int(dist)}cm: {int(dist_correct)}/{int(dist_total)} = {dist_acc*100:.1f}%")

# ============================================================================
# ã€STFTå¿ƒç‡ä¼°è®¡è¯„ä¼°ã€‘
# ============================================================================

print("\nã€STFT å¿ƒç‡ä¼°è®¡è¯„ä¼°ã€‘")

# RFé”™è¯¯é¢„æµ‹çš„STFTè¯„ä¼°
rf_wrong_df = rf_predictions_df[rf_predictions_df['is_correct'] == False].copy()
cfar_wrong_df = cfar_predictions_df[cfar_predictions_df['is_correct'] == False].copy()

print(f"\nRFé”™è¯¯çª—å£æ•°: {len(rf_wrong_df)}")
print(f"CFARé”™è¯¯çª—å£æ•°: {len(cfar_wrong_df)}")

# ä½¿ç”¨è¯„ä¼°æ¨¡å—
if len(rf_wrong_df) > 0:
    print("\n=== RF é”™è¯¯é¢„æµ‹çš„STFTè¯„ä¼° ===")
    rf_evaluator = STFTDualPathEvaluator(output_dir=os.path.join(OUTPUT_DIR, 'rf_stft_evaluation'))
    rf_eval_results = rf_evaluator.evaluate(rf_predictions_df)

if len(cfar_wrong_df) > 0:
    print("\n=== CFAR é”™è¯¯é¢„æµ‹çš„STFTè¯„ä¼° ===")
    cfar_evaluator = STFTDualPathEvaluator(output_dir=os.path.join(OUTPUT_DIR, 'cfar_stft_evaluation'))
    cfar_eval_results = cfar_evaluator.evaluate(cfar_predictions_df)



# ============================================================================
# ã€ä¸‰æ–¹å¯¹æ¯”å¯è§†åŒ–ã€‘
# ============================================================================

print("\nã€ç”Ÿæˆä¸‰æ–¹å¯¹æ¯”å¯è§†åŒ–ã€‘")

# 0. ğŸ”¥ ç‰¹å¾é‡è¦æ€§å¯è§†åŒ–
def plot_feature_importance(all_feature_importances, feature_names, save_path=None):
    """
    ç»˜åˆ¶éšæœºæ£®æ—ç‰¹å¾é‡è¦æ€§æ’åºæŸ±çŠ¶å›¾ï¼ˆè·¨æ‰€æœ‰foldçš„å¹³å‡ï¼‰
    
    å‚æ•°:
        all_feature_importances: æ‰€æœ‰foldçš„ç‰¹å¾é‡è¦æ€§åˆ—è¡¨ (K x n_features)
        feature_names: ç‰¹å¾åç§°åˆ—è¡¨
        save_path: ä¿å­˜è·¯å¾„
    """
    # è½¬æ¢ä¸ºnumpyæ•°ç»„å¹¶è®¡ç®—å¹³å‡å€¼å’Œæ ‡å‡†å·®
    importances_array = np.array(all_feature_importances)  # (K, n_features)
    mean_importances = np.mean(importances_array, axis=0)
    std_importances = np.std(importances_array, axis=0)
    
    # åˆ›å»ºDataFrameå¹¶æŒ‰é‡è¦æ€§æ’åº
    importance_df = pd.DataFrame({
        'feature': feature_names,
        'importance': mean_importances,
        'std': std_importances
    })
    importance_df = importance_df.sort_values('importance', ascending=False)
    
    # ç»˜åˆ¶æŸ±çŠ¶å›¾
    fig, ax = plt.subplots(figsize=(16, 8))
    
    y_pos = np.arange(len(importance_df))
    
    # ä½¿ç”¨é¢œè‰²æ¸å˜ï¼ˆä»æ·±åˆ°æµ…ï¼‰
    colors = plt.cm.viridis(np.linspace(0.3, 0.9, len(importance_df)))

    bars = ax.barh(y_pos, importance_df['importance'],
                xerr=importance_df['std'],
                color=colors,
                edgecolor='black',
                linewidth=1.2,
                error_kw={'elinewidth': 1.5, 'capsize': 3, 'alpha': 0.7})

    ax.set_yticks(y_pos)
    ax.set_yticklabels(importance_df['feature'], fontsize=18)
    ax.invert_yaxis()  # æœ€é‡è¦çš„ç‰¹å¾åœ¨é¡¶éƒ¨

    ax.set_xlabel('Mean Feature Importance', fontsize=18, fontweight='bold')
    ax.set_ylabel('Features', fontsize=18, fontweight='bold')
    ax.set_title('Random Forest Feature Importance Ranking\n(Averaged across 12 Folds with Standard Deviation)',
                fontsize=20, fontweight='bold', pad=20)

    # æ·»åŠ ç½‘æ ¼
    ax.grid(axis='x', alpha=0.3, linestyle='--')
    ax.set_axisbelow(True)

    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for i, (bar, importance, std) in enumerate(zip(bars, importance_df['importance'], importance_df['std'])):
        width = bar.get_width()
        ax.text(width + std + 0.002, bar.get_y() + bar.get_height()/2,
                f'{importance:.4f}',
                # æ•°å€¼æ ‡ç­¾è®¾ç½®ä¸º 18 å·
                ha='left', va='center', fontsize=18, fontweight='bold')

    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"âœ… ç‰¹å¾é‡è¦æ€§å›¾å·²ä¿å­˜: {save_path}")
    
    plt.close()
    
    return importance_df


# è°ƒç”¨å‡½æ•°ç”Ÿæˆç‰¹å¾é‡è¦æ€§å›¾
if len(all_feature_importances) > 0:
    print("\nç”Ÿæˆç‰¹å¾é‡è¦æ€§æ’åºå›¾...")
    feature_importance_path = os.path.join(OUTPUT_DIR, 'feature_importance_ranking.png')
    importance_df = plot_feature_importance(
        all_feature_importances, 
        FEATURE_NAMES, 
        feature_importance_path
    )
    
    # ä¿å­˜ç‰¹å¾é‡è¦æ€§æ•°æ®åˆ°CSV
    importance_csv_path = os.path.join(OUTPUT_DIR, 'feature_importance.csv')
    importance_df.to_csv(importance_csv_path, index=False)
    print(f"âœ… ç‰¹å¾é‡è¦æ€§æ•°æ®å·²ä¿å­˜: {importance_csv_path}")
    
    # æ‰“å°Top 5ç‰¹å¾
    print("\nğŸ“Š Top 5 æœ€é‡è¦ç‰¹å¾:")
    for idx, row in importance_df.head(5).iterrows():
        print(f"  {idx+1}. {row['feature']}: {row['importance']:.4f} (Â±{row['std']:.4f})")


# 1. åŒå­å›¾æ··æ·†çŸ©é˜µå¯¹æ¯”
def plot_dual_confusion_matrix(rf_predictions_df, cfar_predictions_df, save_path=None):
    """
    ç»˜åˆ¶RFå’ŒCFARçš„æ··æ·†çŸ©é˜µå¯¹æ¯”å›¾ï¼ˆåŒå­å›¾ï¼‰
    
    å‚æ•°:
        rf_predictions_df: RFé¢„æµ‹ç»“æœDataFrame
        cfar_predictions_df: CFARé¢„æµ‹ç»“æœDataFrame
        save_path: ä¿å­˜è·¯å¾„
    """
    # æ”¶é›†æ‰€æœ‰å¯èƒ½çš„Range Bin
    all_true_rb = set(rf_predictions_df['true_rb'].unique())
    all_pred_rb_rf = set(rf_predictions_df['pred_rb'].unique())
    all_pred_rb_cfar = set(cfar_predictions_df['pred_rb'].unique())
    
    all_bins = sorted(all_true_rb | all_pred_rb_rf | all_pred_rb_cfar)
    
    # è®¡ç®—RFæ··æ·†çŸ©é˜µ
    rf_cm = confusion_matrix(
        rf_predictions_df['true_rb'], 
        rf_predictions_df['pred_rb'],
        labels=all_bins
    )
    
    # è®¡ç®—CFARæ··æ·†çŸ©é˜µ
    cfar_cm = confusion_matrix(
        cfar_predictions_df['true_rb'], 
        cfar_predictions_df['pred_rb'],
        labels=all_bins
    )
    
    # åˆ›å»ºåŒå­å›¾
    fig, axes = plt.subplots(1, 2, figsize=(20, 8))
    
    # å­å›¾1: RFæ··æ·†çŸ©é˜µ
    # è®¡ç®—RFå‡†ç¡®ç‡ï¼ˆå¯¹è§’çº¿ä¹‹å’Œ / æ€»å’Œï¼‰
    rf_accuracy = np.trace(rf_cm) / np.sum(rf_cm) * 100
    sns.heatmap(rf_cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=all_bins, yticklabels=all_bins,
                cbar_kws={'label': 'Count'}, ax=axes[0],
                linewidths=0.5, linecolor='gray',
                annot_kws={'fontsize': 18})
    axes[0].set_title(f'Random Forest Confusion Matrix\n(Overall Accuracy: {rf_accuracy:.1f}%)', fontsize=20, fontweight='bold', pad=20)
    axes[0].set_xlabel('Predicted Range Bin (1-based)', fontsize=18, fontweight='bold')
    axes[0].set_ylabel('True Range Bin (1-based)', fontsize=18, fontweight='bold')
    # axes[0].text(0.5, 1.05, f'Overall Accuracy: {rf_accuracy:.1f}%', 
    #             ha='center', va='bottom', transform=axes[0].transAxes,
    #             fontsize=14, fontweight='bold', color='darkblue',
    #             bbox=dict(boxstyle='round,pad=0.5', facecolor='lightblue', alpha=0.3))
    
    # å­å›¾2: CFARæ··æ·†çŸ©é˜µ
    # è®¡ç®—CFARå‡†ç¡®ç‡
    cfar_accuracy = np.trace(cfar_cm) / np.sum(cfar_cm) * 100
    sns.heatmap(cfar_cm, annot=True, fmt='d', cmap='Greens', 
                xticklabels=all_bins, yticklabels=all_bins,
                cbar_kws={'label': 'Count'}, ax=axes[1],
                linewidths=0.5, linecolor='gray',
                annot_kws={'fontsize': 18})
    axes[1].set_title(f'CA-CFAR Confusion Matrix\n(Overall Accuracy: {cfar_accuracy:.1f}%)', fontsize=20, fontweight='bold', pad=20)
    axes[1].set_xlabel('Predicted Range Bin (1-based)', fontsize=18, fontweight='bold')
    axes[1].set_ylabel('True Range Bin (1-based)', fontsize=18, fontweight='bold')
    # axes[1].text(0.5, 1.05, f'Overall Accuracy: {cfar_accuracy:.1f}%', 
    #             ha='center', va='bottom', transform=axes[1].transAxes,
    #             fontsize=14, fontweight='bold', color='darkgreen',
    #             bbox=dict(boxstyle='round,pad=0.5', facecolor='lightgreen', alpha=0.3))
    
    # æ€»æ ‡é¢˜
    fig.suptitle('Range Bin Prediction: Random Forest vs CA-CFAR\nLeave-One-File-Out Cross-Validation (K=12)', 
                 fontsize=18, fontweight='bold', y=1.02)
    
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"âœ… åŒå­å›¾æ··æ·†çŸ©é˜µå·²ä¿å­˜: {save_path}")
    
    plt.close()
    
    return rf_cm, cfar_cm, rf_accuracy, cfar_accuracy


print("\nç”ŸæˆåŒå­å›¾æ··æ·†çŸ©é˜µ...")
dual_cm_path = os.path.join(OUTPUT_DIR, 'confusion_matrix_rf_vs_cfar.png')
rf_cm, cfar_cm, rf_cm_acc, cfar_cm_acc = plot_dual_confusion_matrix(
    rf_predictions_df, 
    cfar_predictions_df, 
    dual_cm_path
)


# 2. Range Biné¢„æµ‹å‡†ç¡®ç‡å¯¹æ¯”
fig, ax = plt.subplots(figsize=(12, 6))

methods = ['RF', 'CFAR']
overall_accs = [rf_overall_accuracy * 100, cfar_overall_accuracy * 100]

x = np.arange(len(methods))
bars = ax.bar(x, overall_accs, width=0.6, color=['#FF6B6B', '#4ECDC4'], edgecolor='black', linewidth=1.5)

for bar, acc in zip(bars, overall_accs):
    height = bar.get_height()
    ax.text(bar.get_x() + bar.get_width()/2., height + 1,
            f'{acc:.1f}%',
            ha='center', va='bottom', fontsize=14, fontweight='bold')

ax.set_ylim([0, 105])
ax.set_xticks(x)
ax.set_xticklabels(methods, fontsize=13)
ax.set_ylabel('Overall Accuracy (%)', fontsize=13, fontweight='bold')
ax.set_title('Range Bin Prediction Accuracy Comparison', fontsize=15, fontweight='bold')
ax.grid(axis='y', alpha=0.3, linestyle='--')
ax.set_axisbelow(True)

plt.tight_layout()
plt.savefig(os.path.join(OUTPUT_DIR, 'method_comparison_overall.png'), dpi=300)
plt.close()
print(f"âœ… æ•´ä½“å‡†ç¡®ç‡å¯¹æ¯”å›¾å·²ä¿å­˜")


# 3. æŒ‰è·ç¦»åˆ†ç»„å¯¹æ¯”
distances = sorted(fold_summary_df['distance'].unique())
rf_dist_accs = []
cfar_dist_accs = []

for dist in distances:
    dist_data = fold_summary_df[fold_summary_df['distance'] == dist]
    
    rf_acc = dist_data['rf_correct'].sum() / dist_data['n_test_windows'].sum() * 100
    cfar_acc = dist_data['cfar_correct'].sum() / dist_data['n_test_windows'].sum() * 100
    
    rf_dist_accs.append(rf_acc)
    cfar_dist_accs.append(cfar_acc)

x = np.arange(len(distances))
width = 0.35

fig, ax = plt.subplots(figsize=(12, 6))
bars1 = ax.bar(x - width/2, rf_dist_accs, width, label='RF', color='#FF6B6B', edgecolor='black')
bars2 = ax.bar(x + width/2, cfar_dist_accs, width, label='CFAR', color='#4ECDC4', edgecolor='black')

for bars in [bars1, bars2]:
    for bar in bars:
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height + 1,
                f'{height:.1f}%',
                ha='center', va='bottom', fontsize=10, fontweight='bold')

ax.set_ylim([0, 105])
ax.set_xticks(x)
ax.set_xticklabels([f'{int(d)}cm' for d in distances], fontsize=12)
ax.set_ylabel('Accuracy (%)', fontsize=13, fontweight='bold')
ax.set_xlabel('Distance', fontsize=13, fontweight='bold')
ax.set_title('Range Bin Prediction Accuracy by Distance', fontsize=15, fontweight='bold')
ax.legend(fontsize=12)
ax.grid(axis='y', alpha=0.3, linestyle='--')
ax.set_axisbelow(True)

plt.tight_layout()
plt.savefig(os.path.join(OUTPUT_DIR, 'method_comparison_by_distance.png'), dpi=300)
plt.close()
print(f"âœ… æŒ‰è·ç¦»å¯¹æ¯”å›¾å·²ä¿å­˜")

print(f"\nâœ… æ‰€æœ‰å¯è§†åŒ–å›¾è¡¨å·²ç”Ÿæˆï¼")




## ğŸ“Š è¾“å‡ºæ•ˆæœ
"""
è¿è¡Œåä¼šç”Ÿæˆï¼š

results_rf_cfar_stft/
â”œâ”€â”€ feature_importance_ranking.png   # ğŸ”¥ æ–°å¢ï¼šç‰¹å¾é‡è¦æ€§æ’åºå›¾
â”œâ”€â”€ feature_importance.csv           # ğŸ”¥ æ–°å¢ï¼šç‰¹å¾é‡è¦æ€§æ•°æ®
â”œâ”€â”€ confusion_matrix_rf_vs_cfar.png  # åŒå­å›¾æ··æ·†çŸ©é˜µ
â”œâ”€â”€ method_comparison_overall.png    # æ•´ä½“å‡†ç¡®ç‡å¯¹æ¯”
â”œâ”€â”€ method_comparison_by_distance.png # æŒ‰è·ç¦»å¯¹æ¯”
â””â”€â”€ ...
```

---

## ğŸ“ˆ å›¾è¡¨ç‰¹ç‚¹

**ç‰¹å¾é‡è¦æ€§å›¾**ï¼š
- **æ¨ªå‘æŸ±çŠ¶å›¾**ï¼šæ˜“äºé˜…è¯»ç‰¹å¾åç§°
- **é¢œè‰²æ¸å˜**ï¼šä»é‡è¦åˆ°ä¸é‡è¦ç”¨æ¸å˜è‰²åŒºåˆ†
- **è¯¯å·®çº¿**ï¼šæ˜¾ç¤º12ä¸ªfoldä¹‹é—´çš„æ ‡å‡†å·®
- **æ•°å€¼æ ‡ç­¾**ï¼šç²¾ç¡®æ˜¾ç¤ºæ¯ä¸ªç‰¹å¾çš„é‡è¦æ€§å¾—åˆ†
- **æ’åº**ï¼šä»ä¸Šåˆ°ä¸‹æŒ‰é‡è¦æ€§é™åºæ’åˆ—

**ç»ˆç«¯è¾“å‡ºç¤ºä¾‹**ï¼š

ğŸ“Š Top 5 æœ€é‡è¦ç‰¹å¾:
  1. amp_snr_db: 0.1523 (Â±0.0089)
  2. amp_energy_cardiac: 0.1247 (Â±0.0156)
  3. phase_snr_db: 0.1098 (Â±0.0123)
  4. amp_hr_peak_purity: 0.0876 (Â±0.0098)
  5. amp_purity_ratio: 0.0754 (Â±0.0112)
  """