## --------------------------------------------------------------##
# v1 å¯¹æ¯ä¸ªå•å…ƒæ ¼è¿›è¡ŒäºŒåˆ†ç±»ï¼ˆæ˜¯å¦æ˜¯äººä½“ï¼‰ï¼Œæš‚æ—¶ä¸è€ƒè™‘RXï¼ŒæŒ‡å®šRX=1
# åç»­å¯ä»¥è€ƒè™‘RXçš„ç‰¹å¾èåˆ
## v2ç›¸æ¯”v1ï¼Œå¼ºåˆ¶æ¯ä¸ª30ç§’ç‰‡æ®µRXé€šé“è¾“å‡ºæœ‰ä¸”ä»…æœ‰ä¸€ä¸ªäººä½“æ‰€åœ¨å•å…ƒæ ¼
## v3ç›¸æ¯”v2ï¼Œä¿å­˜æ¨¡å‹ä»¥ä¾¿åç»­è°ƒç”¨
## v4ç›¸æ¯”v3ï¼Œä¿®æ”¹è·ç¦»å’Œrange bin indexçš„å¯¹åº”å…³ç³»ï¼Œ40/50/60cmå¯¹åº”range bin index 4/5/6(base 1)
## v5ç›¸æ¯”v4ï¼Œå¢åŠ range bin çº§åˆ«çš„confusion matrixç»˜åˆ¶
## v6ç›¸æ¯”v5ï¼šK-Fold äº¤å‰éªŒè¯ç‰ˆæœ¬
# ä¸»è¦æ”¹åŠ¨ï¼šä½¿ç”¨ StratifiedGroupKFold è¿›è¡Œ K-Fold äº¤å‰éªŒè¯
## v8æ”¹è‡ªv6ï¼Œä¸»è¦å¢åŠ æŒ‰è·ç¦»è¾“å‡ºç‰¹å¾å…·ä½“çš„å€¼ï¼ˆç”¨äºè¯„ä¼°æ•°æ®è´¨é‡ï¼‰å’Œæ•°æ®è´¨é‡è¯„ä¼°å‡½æ•°Qçš„å¾—åˆ†
## --------------------------------------------------------------##
"""
éšæœºæ£®æ— Range Bin åˆ†ç±»å™¨ - K-Fold äº¤å‰éªŒè¯ç‰ˆ
ä½¿ç”¨ K-Fold äº¤å‰éªŒè¯ï¼Œå¯ä»¥åœ¨å…¨éƒ¨æ•°æ®ä¸Šè¿›è¡Œæµ‹è¯•
"""

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import StratifiedGroupKFold  # ğŸ”¥ æ”¹ä¸º K-Fold
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (
    classification_report, confusion_matrix, roc_auc_score, 
    roc_curve, precision_recall_curve, f1_score, accuracy_score
)
import warnings
warnings.filterwarnings('ignore')
import joblib
import json
from datetime import datetime

# å¯¼å…¥ç‰¹å¾æå–æ¨¡å—
from FeatureExtract_v6 import extract_features_from_all_files

# ============================================================================
# ã€é…ç½®åŒºã€‘
# ============================================================================
print("=" * 70)
print("ğŸš€ éšæœºæ£®æ—Range Biné¢„æµ‹ç³»ç»Ÿ - K-Fold äº¤å‰éªŒè¯ç‰ˆ")
print("=" * 70)

# --- 1. å®éªŒæ–‡ä»¶é…ç½® ---
FILE_CONFIGS = [
    # 40cm - RB Index 6
    {'path': r"D:\MSc\Dissertation\Data\250902\test2\RangeFFT\RangeBin6_RX1\test2_NeuLogRadar_aligned_trimmed.mat", 'distance': 40, 'rb_index': 6, 'rx_index_example': 1},
    {'path': r"D:\MSc\Dissertation\Data\250902\test3\RangeFFT\RangeBin6_RX1\test3_NeuLogRadar_aligned_trimmed.mat", 'distance': 40, 'rb_index': 6, 'rx_index_example': 1},
    {'path': r"D:\MSc\Dissertation\Data\250902\test4\RangeFFT\RangeBin6_RX1\test4_NeuLogRadar_aligned_trimmed.mat", 'distance': 40, 'rb_index': 6, 'rx_index_example': 1},
    {'path': r"D:\MSc\Dissertation\Data\250902\test5\RangeFFT\RangeBin6_RX1\test5_NeuLogRadar_aligned_trimmed.mat", 'distance': 40, 'rb_index': 6, 'rx_index_example': 1},
    
    # 50cm - RB Index 7
    {'path': r"D:\MSc\Dissertation\Data\250826\test8\RangeFFT\RangeBin7_RX1\test8_NeuLogRadar_aligned_trimmed.mat", 'distance': 50, 'rb_index': 7, 'rx_index_example': 1},
    {'path': r"D:\MSc\Dissertation\Data\250826\test9\RangeFFT\RangeBin7_RX1\test9_NeuLogRadar_aligned_trimmed.mat", 'distance': 50, 'rb_index': 7, 'rx_index_example': 1},
    {'path': r"D:\MSc\Dissertation\Data\250925\test1\RangeFFT\RangeBin7_RX2\test1_NeuLogRadar_aligned_trimmed.mat", 'distance': 50, 'rb_index': 7, 'rx_index_example': 2},
    {'path': r"D:\MSc\Dissertation\Data\250925\test2\RangeFFT\RangeBin7_RX2\test2_NeuLogRadar_aligned_trimmed.mat", 'distance': 50, 'rb_index': 7, 'rx_index_example': 2},

    # 60cm - RB Index 8
    {'path': r"D:\MSc\Dissertation\Data\250925\test3\RangeFFT\RangeBin8_RX4\test3_NeuLogRadar_aligned_trimmed.mat", 'distance': 60, 'rb_index': 8, 'rx_index_example': 4},
    {'path': r"D:\MSc\Dissertation\Data\250925\test4\RangeFFT\RangeBin8_RX1\test4_NeuLogRadar_aligned_trimmed.mat", 'distance': 60, 'rb_index': 8, 'rx_index_example': 1},
    {'path': r"D:\MSc\Dissertation\Data\251016\test1\RangeFFT\RangeBin8_RX1\test1_NeuLogRadar_aligned_trimmed.mat", 'distance': 60, 'rb_index': 8, 'rx_index_example': 1},
    {'path': r"D:\MSc\Dissertation\Data\251016\test2\RangeFFT\RangeBin8_RX2\test2_NeuLogRadar_aligned_trimmed.mat", 'distance': 60, 'rb_index': 8, 'rx_index_example': 2},
]

# --- 2. æ¨¡å‹å‚æ•°é…ç½® ---
WINDOW_DURATION_S = 30  # çª—å£é•¿åº¦ï¼ˆç§’ï¼‰
TARGET_RX_INDEX = 1  # å…³æ³¨çš„RXå¤©çº¿ç´¢å¼•ï¼ˆ1-basedï¼‰
N_SPLITS = 4  # ğŸ”¥ K-Fold çš„ K å€¼ï¼ˆ3 æˆ– 4ï¼‰
RANDOM_STATE = 42
N_JOBS = -1

# --- 3. ç‰¹å¾åç§° ---
# --- 3. ç‰¹å¾åç§° ---
FEATURE_NAMES = [
    'amp_mean', 'amp_std', 'amp_p2p', 'amp_skewness', 'amp_kurtosis',
    'phase_diff_std', 'phase_diff_range',
    'amp_energy_resp', 'amp_energy_cardiac', 'amp_life_energy_ratio',
    'amp_freq_peak_pos', 'amp_purity_ratio', 'amp_energy_ratio_C_R', 'amp_snr_db',
    'phase_energy_ratio_C_R', 'phase_snr_db',
    'amp_hr_peak_purity' # <--- æ–°å¢
]

# ğŸ”¥ æ¨¡å‹ä¿å­˜é…ç½®
MODEL_SAVE_DIR = f'./RandomForest_1125_v8'
MODEL_NAME = 'rf_rangebin_classifier'

if not os.path.exists(MODEL_SAVE_DIR):
    os.makedirs(MODEL_SAVE_DIR)
    print(f"âœ… å·²åˆ›å»ºæ¨¡å‹ä¿å­˜ç›®å½•: {MODEL_SAVE_DIR}")

# è®¾ç½®ä¸­æ–‡å­—ä½“æ”¯æŒ
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

# ============================================================================
# ã€Step 0: è°ƒç”¨ç‰¹å¾æå–æ¨¡å—ã€‘
# ============================================================================
print("\nã€Step 0ã€‘ è°ƒç”¨ç‰¹å¾æå–æ¨¡å—...")
print(f"çª—å£é•¿åº¦: {WINDOW_DURATION_S} ç§’")
print(f"å®éªŒæ–‡ä»¶æ•°: {len(FILE_CONFIGS)}")

ALL_SEGMENTS_WITH_FEATURES = extract_features_from_all_files(
    FILE_CONFIGS, 
    window_duration_s=WINDOW_DURATION_S
)

if len(ALL_SEGMENTS_WITH_FEATURES) == 0:
    raise RuntimeError("âŒ ç‰¹å¾æå–å¤±è´¥ï¼Œæ²¡æœ‰ç”Ÿæˆä»»ä½•ç‰‡æ®µï¼")

print(f"\nâœ… ç‰¹å¾æå–å®Œæˆï¼Œå…±ç”Ÿæˆ {len(ALL_SEGMENTS_WITH_FEATURES)} ä¸ªç‰¹å¾ç‰‡æ®µã€‚")

# ============================================================================
# ã€Step 1: æ•°æ®å‡†å¤‡ - æ„å»ºæ ·æœ¬ã€‘
# ============================================================================
print("\nã€Step 1ã€‘ æ„å»ºäºŒåˆ†ç±»æ ·æœ¬...")

def build_samples_from_segments(segments_list, target_rx_index):
    """ä»ç‰‡æ®µåˆ—è¡¨æ„å»ºäºŒåˆ†ç±»æ ·æœ¬"""
    samples_data = []
    
    for seg_idx, segment in enumerate(segments_list):
        file_name = segment['original_file']
        segment_id = segment['segment_index']
        heart_count = segment['heart_count']
        range_one_hot = segment['range_one_hot'].squeeze()
        
        true_rb_index = np.argmax(range_one_hot) + 1
        
        rb_idx_0based = segment.get('rb_index_1based', true_rb_index) - 1
        if rb_idx_0based == 5:  # ä¿®æ­£ï¼š40cm å¯¹åº” rb_index=6 (0based=5)
            distance = 40
        elif rb_idx_0based == 6:
            distance = 50
        elif rb_idx_0based == 7:
            distance = 60
        else:
            distance = 50
        
        rx_idx_0based = target_rx_index - 1
        R = segment['amp_mean'].shape[0]
        
        for r in range(R):
            rb_1based = r + 1
            
            feature_vector = []
            for feat_name in FEATURE_NAMES:
                if feat_name in segment:
                    feat_value = segment[feat_name][r, rx_idx_0based]
                    feature_vector.append(feat_value)
                else:
                    feature_vector.append(0.0)
            
            label = 1 if range_one_hot[r] == 1 else 0
            
            samples_data.append({
                'features': feature_vector,
                'label': label,
                'file_name': file_name,
                'segment_id': segment_id,
                'rb_index': rb_1based,
                'distance': distance,
                'heart_count': heart_count,
                'true_rb_index': true_rb_index,
                'segment_global_id': seg_idx
            })
    
    df = pd.DataFrame(samples_data)
    X = np.array(df['features'].tolist())
    y = df['label'].values
    meta_df = df.drop(columns=['features', 'label'])
    
    return X, y, meta_df

X_raw, y_raw, meta_info = build_samples_from_segments(
    ALL_SEGMENTS_WITH_FEATURES, 
    TARGET_RX_INDEX
)

print(f"âœ… æ ·æœ¬æ„å»ºå®Œæˆï¼")
print(f"   æ€»æ ·æœ¬æ•°: {len(y_raw)}")
print(f"   ç‰¹å¾ç»´åº¦: {X_raw.shape[1]}")
print(f"   æ­£æ ·æœ¬æ•°: {np.sum(y_raw == 1)} ({np.sum(y_raw == 1)/len(y_raw)*100:.2f}%)")
print(f"   è´Ÿæ ·æœ¬æ•°: {np.sum(y_raw == 0)} ({np.sum(y_raw == 0)/len(y_raw)*100:.2f}%)")

# ============================================================================
# ã€Step 1.5: æ•°æ®è´¨é‡è¯„ä¼°ã€‘ğŸ”¥ æ ¸å¿ƒï¼šä½¿ç”¨ HR Peak Purity ä¿®æ­£ Q åˆ†æ•°
# ============================================================================
print("\nã€Step 1.5ã€‘ æ•°æ®è´¨é‡è¯„ä¼°å’Œç‰¹å¾å€¼è¾“å‡º...")

# --- A. è´¨é‡è¯„åˆ†å‚æ•°å®šä¹‰ ---
# æƒé‡
W_SNR = 0.4
W_PURITY = 0.3
W_HEART_COUNT = 0.3

# å½’ä¸€åŒ–ç›®æ ‡/é˜ˆå€¼
SNR_MIN = 10.0
SNR_MAX = 40.0
HR_PURITY_TARGET = 8.0  # ğŸ”¥ é’ˆå¯¹ HR é¢‘å¸¦é‡æ–°è®¾å®šç›®æ ‡å€¼ (åŸPurity Target=5.0)
HEART_COUNT_MIN = 1 

# --- B. æ•°æ®åˆå¹¶ä¸è®¡ç®— ---

# 1. å°†åŸå§‹ç‰¹å¾æ•°æ®è½¬æ¢ä¸º DataFrame
X_df = pd.DataFrame(X_raw, columns=FEATURE_NAMES)
# ğŸ”¥ å…³é”®ä¿®å¤ï¼šå°†æ ‡ç­¾ y_raw åŠ å…¥åˆ°å…ƒæ•°æ® DataFrame ä¸­
meta_info['label'] = y_raw 
# 2. åˆå¹¶ç‰¹å¾å’Œå…ƒæ•°æ®
quality_df = pd.concat([X_df, meta_info.reset_index(drop=True)], axis=1)

def calculate_quality_score(row):
    """è®¡ç®—å•ä¸ªç‰‡æ®µçš„è´¨é‡åˆ†æ•° Q"""
    
    # 1. SNR å½’ä¸€åŒ– (w1 = 0.4)
    snr_norm = np.clip((row['amp_snr_db'] - SNR_MIN) / (SNR_MAX - SNR_MIN), 0.0, 1.0)
    
    # 2. HR Peak Purity å½’ä¸€åŒ– (w2 = 0.3)
    purity_norm = np.clip(row['amp_hr_peak_purity'] / HR_PURITY_TARGET, 0.0, 1.0) # <--- ä½¿ç”¨æ–°çš„ HR Purity
    
    # 3. Heart Count (w3 = 0.3)
    heart_norm = 1.0 if row['heart_count'] >= HEART_COUNT_MIN else 0.0
    
    # æœ€ç»ˆåˆ†æ•° Q
    Q = W_SNR * snr_norm + W_PURITY * purity_norm + W_HEART_COUNT * heart_norm
    
    return Q

# è®¡ç®— Q åˆ†æ•°
quality_df['Quality_Score_Q'] = quality_df.apply(calculate_quality_score, axis=1)

# --- C. æŒ‰è·ç¦»åˆ†ç»„è¾“å‡ºç»“æœ ---

# é€‰å‡ºè¦å±•ç¤ºçš„å…³é”®ç‰¹å¾ï¼ˆåŒ…æ‹¬ç”¨äºè®¡ç®— Q çš„ç‰¹å¾ï¼‰
KEY_QUALITY_FEATURES = [
    'amp_snr_db', 
    'amp_purity_ratio', # ä¿ç•™åŸæœ‰çš„ Purity ç”¨äºå¯¹æ¯”
    'amp_hr_peak_purity', # <--- æ–°å¢å±•ç¤º
    'amp_energy_resp', 
    'amp_energy_cardiac', 
    'amp_life_energy_ratio',
    'phase_diff_std',
    'amp_mean', 
    'amp_std', 
    'amp_p2p', 
    'amp_skewness', 
    'amp_kurtosis', 
    'phase_diff_range',
    'amp_freq_peak_pos',
    'amp_energy_ratio_C_R',
    'phase_energy_ratio_C_R', 
    'phase_snr_db'
]
DISPLAY_FEATURES = KEY_QUALITY_FEATURES + ['Quality_Score_Q', 'heart_count', 'label'] # å¢åŠ  label ç”¨äºæ–¹ä¾¿èšåˆ

# åˆ†ç»„è®¡ç®—å¹³å‡å€¼å’Œæ ‡å‡†å·® (è®¡ç®—å…¨æ ·æœ¬çš„ mean/std)
grouped_quality = quality_df.groupby('distance')[DISPLAY_FEATURES].agg(['mean', 'std'])

print("\n" + "=" * 100)
print(f"ğŸ“Š æ•°æ®è´¨é‡è¯„ä¼°å’Œå…³é”®ç‰¹å¾å€¼ (N_segments={quality_df['segment_global_id'].nunique()})")
print(f"   è´¨é‡è¯„åˆ† Q å…¬å¼: Q = {W_SNR}*SNR_norm + {W_PURITY}*HR_Purity_norm (Target={HR_PURITY_TARGET}) + {W_HEART_COUNT}*HeartCount_binary")
print("=" * 100)
# ... (å…¶ä½™è¾“å‡ºä»£ç ä¿æŒä¸å˜)

for dist in sorted(quality_df['distance'].unique()):
    # ğŸ”¥ æ­£æ ·æœ¬ (ç›®æ ‡ Range Bin) ç­›é€‰
    dist_samples_df = quality_df[
        (quality_df['distance'] == dist) & (quality_df['label'] == 1)
    ].reset_index(drop=True)
    
    # å…¨æ ·æœ¬ç­›é€‰
    dist_all_samples_df = quality_df[quality_df['distance'] == dist]
    
    n_samples = dist_all_samples_df.shape[0]
    n_segments = dist_all_samples_df['segment_global_id'].nunique()
    
    print(f"\n--- è·ç¦» {dist}cm æ±‡æ€» ({n_segments} ä¸ªç‰‡æ®µ, {n_samples} ä¸ªæ ·æœ¬) ---")
    
    if dist_samples_df.empty:
        print("æ— æ­£æ ·æœ¬ï¼ˆç›®æ ‡ Range Bin æ ·æœ¬ï¼‰å¯ä¾›å±•ç¤ºã€‚")
        continue

    # åªå±•ç¤ºç›®æ ‡ Range Bin çš„å¹³å‡ç‰¹å¾å€¼ (label=1)
    rb_mean = dist_samples_df[DISPLAY_FEATURES].mean().to_dict()
    
    print(f"{'æŒ‡æ ‡åç§°':<25} | {'ç›®æ ‡RBå¹³å‡å€¼ (label=1)':<25} | {'å…¨æ ·æœ¬å¹³å‡å€¼ (label=0+1)':<25}")
    print("-" * 100)
    
    # æ‰“å° Q åˆ†æ•°
    q_score_mean_rb = dist_samples_df['Quality_Score_Q'].mean()
    q_score_std_rb = dist_samples_df['Quality_Score_Q'].std()
    q_score_mean_all = dist_all_samples_df['Quality_Score_Q'].mean()
    q_score_std_all = dist_all_samples_df['Quality_Score_Q'].std()
    
    print(f"{'Quality_Score_Q':<25} | {q_score_mean_rb:.4f} Â± {q_score_std_rb:.4f} | {q_score_mean_all:.4f} Â± {q_score_std_all:.4f}")
    
    # æ‰“å°å…¶ä»–å…³é”®æŒ‡æ ‡
    for feat in KEY_QUALITY_FEATURES:
        mean_val_rb = rb_mean[feat]
        std_val_rb = dist_samples_df[feat].std()
        
        mean_val_all = dist_all_samples_df[feat].mean()
        std_val_all = dist_all_samples_df[feat].std()
        
        # ç¡®ä¿æ ‡å‡†å·®ä¸æ˜¯ NaN (å¯¹äºåªæœ‰ä¸€ä¸ªæ ·æœ¬çš„æƒ…å†µ)
        std_val_rb = std_val_rb if not np.isnan(std_val_rb) else 0.0
        std_val_all = std_val_all if not np.isnan(std_val_all) else 0.0
        
        print(f"{feat:<25} | {mean_val_rb:.6e} Â± {std_val_rb:.4e} | {mean_val_all:.6e} Â± {std_val_all:.4e}")

print("=" * 100)
print(f"âœ… æ•°æ®è´¨é‡è¯„ä¼°è¾“å‡ºå®Œæˆã€‚")

# ============================================================================
# ã€Step 2: K-Fold äº¤å‰éªŒè¯ã€‘ğŸ”¥ æ ¸å¿ƒä¿®æ”¹
# ============================================================================
print(f"\nã€Step 2ã€‘ {N_SPLITS}-Fold äº¤å‰éªŒè¯...")

# åˆ›å»ºåˆ†å±‚æ ‡ç­¾
stratify_labels = meta_info['distance'].astype(str) + '_' + y_raw.astype(str)
groups = meta_info['segment_global_id'].values

# ğŸ”¥ ä½¿ç”¨ StratifiedGroupKFold
skf = StratifiedGroupKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)

# å­˜å‚¨æ‰€æœ‰ fold çš„ç»“æœ
all_fold_results = []
all_test_predictions = []  # å­˜å‚¨æ‰€æœ‰æµ‹è¯•é›†çš„é¢„æµ‹ï¼ˆç”¨äºåç»­æ±‡æ€»ï¼‰
fold_models = []  # å­˜å‚¨æ¯ä¸ª fold çš„æ¨¡å‹

print(f"\nå¼€å§‹ {N_SPLITS}-Fold äº¤å‰éªŒè¯...")

for fold_idx, (train_idx, test_idx) in enumerate(skf.split(X_raw, stratify_labels, groups=groups)):
    print("\n" + "=" * 70)
    print(f"ğŸ“Š Fold {fold_idx + 1}/{N_SPLITS}")
    print("=" * 70)
    
    # åˆ’åˆ†æ•°æ®
    X_train_raw = X_raw[train_idx]
    X_test_raw = X_raw[test_idx]
    y_train = y_raw[train_idx]
    y_test = y_raw[test_idx]
    meta_train = meta_info.iloc[train_idx].reset_index(drop=True)
    meta_test = meta_info.iloc[test_idx].reset_index(drop=True)
    
    train_segments = set(meta_train['segment_global_id'].unique())
    test_segments = set(meta_test['segment_global_id'].unique())
    
    print(f"   è®­ç»ƒé›†: {len(y_train)} æ ·æœ¬, {len(train_segments)} ç‰‡æ®µ")
    print(f"   æµ‹è¯•é›†: {len(y_test)} æ ·æœ¬, {len(test_segments)} ç‰‡æ®µ")
    
    # éªŒè¯æ— æ³„éœ²
    assert len(train_segments & test_segments) == 0, "âŒ è®­ç»ƒé›†å’Œæµ‹è¯•é›†æœ‰é‡å ç‰‡æ®µï¼"
    
    # ç‰¹å¾æ ‡å‡†åŒ–
    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train_raw)
    X_test = scaler.transform(X_test_raw)
    
    # è®­ç»ƒéšæœºæ£®æ—ï¼ˆä½¿ç”¨å›ºå®šå‚æ•°ï¼Œæˆ–è€…ä½ å¯ä»¥åŠ å…¥ç½‘æ ¼æœç´¢ï¼‰
    rf_model = RandomForestClassifier(
        n_estimators=200,
        max_depth=20,
        min_samples_split=5,
        min_samples_leaf=2,
        class_weight='balanced',
        random_state=RANDOM_STATE,
        n_jobs=N_JOBS
    )
    
    rf_model.fit(X_train, y_train)
    
    # æµ‹è¯•é›†é¢„æµ‹
    y_test_pred = rf_model.predict(X_test)
    y_test_proba = rf_model.predict_proba(X_test)[:, 1]
    
    # è¯„ä¼°å•å…ƒæ ¼çº§æ€§èƒ½
    test_auc = roc_auc_score(y_test, y_test_proba)
    test_f1 = f1_score(y_test, y_test_pred)
    
    print(f"   å•å…ƒæ ¼çº§ AUC: {test_auc:.4f}")
    print(f"   å•å…ƒæ ¼çº§ F1: {test_f1:.4f}")
    
    # Range Bin çº§åˆ«è¯„ä¼°
    def evaluate_range_bin_level(y_true, y_proba, meta_df):
        """å¼ºåˆ¶æ¯ç‰‡æ®µé€‰æ‹©ä¸€ä¸ª Range Bin"""
        segment_results = []
        
        for seg_id in meta_df['segment_global_id'].unique():
            seg_mask = meta_df['segment_global_id'] == seg_id
            seg_data = meta_df[seg_mask].iloc[0]
            
            seg_probas = y_proba[seg_mask]
            seg_rb_indices = meta_df[seg_mask]['rb_index'].values
            
            max_idx = np.argmax(seg_probas)
            pred_rb = seg_rb_indices[max_idx]
            true_rb = seg_data['true_rb_index']
            
            segment_results.append({
                'fold': fold_idx + 1,
                'file_name': seg_data['file_name'],
                'segment_id': seg_data['segment_id'],
                'distance': seg_data['distance'],
                'true_rb': true_rb,
                'pred_rb': pred_rb,
                'correct': (pred_rb == true_rb),
                'error': abs(pred_rb - true_rb),
                'max_proba': seg_probas[max_idx],
                'segment_global_id': seg_id
            })
        
        return pd.DataFrame(segment_results)
    
    rb_results = evaluate_range_bin_level(y_test, y_test_proba, meta_test)
    rb_accuracy = rb_results['correct'].mean()
    avg_error = rb_results['error'].mean()
    
    print(f"   Range Bin Top-1 å‡†ç¡®ç‡: {rb_accuracy:.2%}")
    print(f"   å¹³å‡è·ç¦»è¯¯å·®: {avg_error:.2f} Bins")
    
    # ä¿å­˜è¿™ä¸ª fold çš„ç»“æœ
    fold_result = {
        'fold': fold_idx + 1,
        'n_train': len(y_train),
        'n_test': len(y_test),
        'n_train_segments': len(train_segments),
        'n_test_segments': len(test_segments),
        'cell_auc': test_auc,
        'cell_f1': test_f1,
        'rb_accuracy': rb_accuracy,
        'rb_avg_error': avg_error
    }
    all_fold_results.append(fold_result)
    
    # ä¿å­˜æµ‹è¯•é›†é¢„æµ‹ï¼ˆç”¨äºæ±‡æ€»ï¼‰
    test_predictions = meta_test.copy()
    test_predictions['y_true'] = y_test
    test_predictions['y_pred'] = y_test_pred
    test_predictions['y_proba'] = y_test_proba
    test_predictions['fold'] = fold_idx + 1
    all_test_predictions.append(test_predictions)
    
    # ä¿å­˜æ¨¡å‹å’Œ scaler
    fold_models.append({
        'model': rf_model,
        'scaler': scaler,
        'fold': fold_idx + 1
    })

# ============================================================================
# ã€Step 3: æ±‡æ€»æ‰€æœ‰ Fold çš„ç»“æœã€‘ğŸ”¥ æ ¸å¿ƒä¼˜åŠ¿
# ============================================================================
print("\n" + "=" * 70)
print("ğŸ“Š æ±‡æ€»æ‰€æœ‰ Fold çš„ç»“æœ")
print("=" * 70)

# æ±‡æ€» fold æ€§èƒ½
fold_summary_df = pd.DataFrame(all_fold_results)
print("\nå„ Fold æ€§èƒ½:")
print(fold_summary_df.to_string(index=False))

print(f"\nå¹³å‡æ€§èƒ½ (è·¨ {N_SPLITS} Folds):")
print(f"   å•å…ƒæ ¼çº§ AUC: {fold_summary_df['cell_auc'].mean():.4f} Â± {fold_summary_df['cell_auc'].std():.4f}")
print(f"   å•å…ƒæ ¼çº§ F1: {fold_summary_df['cell_f1'].mean():.4f} Â± {fold_summary_df['cell_f1'].std():.4f}")
print(f"   Range Bin å‡†ç¡®ç‡: {fold_summary_df['rb_accuracy'].mean():.2%} Â± {fold_summary_df['rb_accuracy'].std():.2%}")
print(f"   å¹³å‡è·ç¦»è¯¯å·®: {fold_summary_df['rb_avg_error'].mean():.2f} Â± {fold_summary_df['rb_avg_error'].std():.2f} Bins")

# ğŸ”¥ åˆå¹¶æ‰€æœ‰æµ‹è¯•é›†é¢„æµ‹ï¼ˆè¿™æ ·å°±æœ‰å…¨éƒ¨æ•°æ®çš„é¢„æµ‹äº†ï¼ï¼‰
all_predictions_df = pd.concat(all_test_predictions, ignore_index=True)

print(f"\nâœ… æ±‡æ€»å®Œæˆï¼")
print(f"   æ€»é¢„æµ‹æ ·æœ¬æ•°: {len(all_predictions_df)}")
print(f"   è¦†ç›–çš„ç‰‡æ®µæ•°: {all_predictions_df['segment_global_id'].nunique()}")

# è®¡ç®—å…¨å±€ Range Bin å‡†ç¡®ç‡
def evaluate_all_range_bins(predictions_df):
    """åŸºäºæ‰€æœ‰é¢„æµ‹è®¡ç®— Range Bin å‡†ç¡®ç‡"""
    segment_results = []
    
    for seg_id in predictions_df['segment_global_id'].unique():
        seg_data = predictions_df[predictions_df['segment_global_id'] == seg_id]
        
        seg_probas = seg_data['y_proba'].values
        seg_rb_indices = seg_data['rb_index'].values
        
        max_idx = np.argmax(seg_probas)
        pred_rb = seg_rb_indices[max_idx]
        true_rb = seg_data['true_rb_index'].iloc[0]
        
        segment_results.append({
            'file_name': seg_data['file_name'].iloc[0],
            'segment_id': seg_data['segment_id'].iloc[0],
            'distance': seg_data['distance'].iloc[0],
            'true_rb': true_rb,
            'pred_rb': pred_rb,
            'correct': (pred_rb == true_rb),
            'error': abs(pred_rb - true_rb),
            'fold': seg_data['fold'].iloc[0]
        })
    
    return pd.DataFrame(segment_results)

global_rb_results = evaluate_all_range_bins(all_predictions_df)

print("\n" + "=" * 70)
print("ğŸ¯ å…¨å±€ Range Bin æ€§èƒ½ï¼ˆåŸºäºå…¨éƒ¨æ•°æ®ï¼‰:")
print("=" * 70)
print(f"   Top-1 å‡†ç¡®ç‡: {global_rb_results['correct'].mean():.2%}")
print(f"   å¹³å‡è·ç¦»è¯¯å·®: {global_rb_results['error'].mean():.2f} Bins")
print(f"   æŒ‰è·ç¦»åˆ†ç»„å‡†ç¡®ç‡:")
for dist in sorted(global_rb_results['distance'].unique()):
    dist_acc = global_rb_results[global_rb_results['distance'] == dist]['correct'].mean()
    n_segments = len(global_rb_results[global_rb_results['distance'] == dist])
    print(f"     {dist}cm: {dist_acc:.2%} ({n_segments} ç‰‡æ®µ)")

# ============================================================================
# ã€Step 4: ä¿å­˜ç»“æœã€‘
# ============================================================================
print("\nã€Step 4ã€‘ ä¿å­˜ç»“æœ...")

timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')

# ä¿å­˜ fold æ±‡æ€»
fold_summary_df.to_csv(
    os.path.join(MODEL_SAVE_DIR, f'fold_summary_{timestamp}.csv'), 
    index=False
)

# ä¿å­˜å…¨å±€ Range Bin ç»“æœ
global_rb_results.to_csv(
    os.path.join(MODEL_SAVE_DIR, f'global_rb_results_{timestamp}.csv'), 
    index=False
)

# ä¿å­˜æ‰€æœ‰é¢„æµ‹ï¼ˆç”¨äºåç»­å¿ƒç‡ä¼°è®¡ï¼‰
all_predictions_df.to_csv(
    os.path.join(MODEL_SAVE_DIR, f'all_predictions_{timestamp}.csv'), 
    index=False
)

# ä¿å­˜æ¯ä¸ª fold çš„æ¨¡å‹
for fold_data in fold_models:
    fold_num = fold_data['fold']
    joblib.dump(
        fold_data['model'], 
        os.path.join(MODEL_SAVE_DIR, f'rf_model_fold{fold_num}_{timestamp}.pkl')
    )
    joblib.dump(
        fold_data['scaler'], 
        os.path.join(MODEL_SAVE_DIR, f'scaler_fold{fold_num}_{timestamp}.pkl')
    )

# ä¿å­˜å…ƒä¿¡æ¯
metadata = {
    'model_name': MODEL_NAME,
    'timestamp': timestamp,
    'n_splits': N_SPLITS,
    'feature_names': FEATURE_NAMES,
    'window_duration_s': WINDOW_DURATION_S,
    'random_state': RANDOM_STATE,
    'global_rb_accuracy': float(global_rb_results['correct'].mean()),
    'global_rb_avg_error': float(global_rb_results['error'].mean()),
    'fold_performance': fold_summary_df.to_dict('records')
}

with open(os.path.join(MODEL_SAVE_DIR, f'metadata_{timestamp}.json'), 'w') as f:
    json.dump(metadata, f, indent=4)

print(f"âœ… æ‰€æœ‰ç»“æœå·²ä¿å­˜è‡³: {MODEL_SAVE_DIR}")

# ============================================================================
# ã€Step 5: å¯è§†åŒ–ã€‘
# ============================================================================
print("\nã€Step 5ã€‘ ç”Ÿæˆå¯è§†åŒ–...")

# 1. å…¨å±€æ··æ·†çŸ©é˜µï¼ˆRange Bin çº§åˆ«ï¼‰
true_rb_indices = global_rb_results['true_rb'].values
pred_rb_indices = global_rb_results['pred_rb'].values
all_rb_indices = np.arange(1, 22)

cm_rb = confusion_matrix(true_rb_indices, pred_rb_indices, labels=all_rb_indices)

plt.figure(figsize=(12, 10))
sns.heatmap(cm_rb, annot=True, fmt='d', cmap='Blues', 
            xticklabels=all_rb_indices, yticklabels=all_rb_indices)
plt.xlabel('Predicted Range Bin (base-1)', fontsize=14)
plt.ylabel('True Range Bin (base-1)', fontsize=14)
plt.title(f'Confusion Matrix - {N_SPLITS}-Fold CV (All Data)', fontsize=16)
plt.tight_layout()
plt.savefig(os.path.join(MODEL_SAVE_DIR, 'confusion_matrix_global.png'), 
            dpi=300, bbox_inches='tight')
plt.close()

# 2. Fold æ€§èƒ½å¯¹æ¯”
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

ax1 = axes[0]
ax1.bar(fold_summary_df['fold'], fold_summary_df['rb_accuracy'] * 100)
ax1.axhline(fold_summary_df['rb_accuracy'].mean() * 100, 
            color='r', linestyle='--', label='Mean')
ax1.set_xlabel('Fold')
ax1.set_ylabel('Range Bin Accuracy (%)')
ax1.set_title('Range Bin Accuracy by Fold')
ax1.legend()
ax1.grid(axis='y', alpha=0.3)

ax2 = axes[1]
ax2.bar(fold_summary_df['fold'], fold_summary_df['rb_avg_error'])
ax2.axhline(fold_summary_df['rb_avg_error'].mean(), 
            color='r', linestyle='--', label='Mean')
ax2.set_xlabel('Fold')
ax2.set_ylabel('Average Error (Bins)')
ax2.set_title('Average Prediction Error by Fold')
ax2.legend()
ax2.grid(axis='y', alpha=0.3)

plt.tight_layout()
plt.savefig(os.path.join(MODEL_SAVE_DIR, 'fold_performance_comparison.png'), 
            dpi=300, bbox_inches='tight')
plt.close()

print(f"âœ… å¯è§†åŒ–å·²ä¿å­˜")

# ============================================================================
# ã€æ€»ç»“ã€‘
# ============================================================================
print("\n" + "=" * 70)
print("âœ… K-Fold äº¤å‰éªŒè¯å®Œæˆï¼")
print("=" * 70)
print(f"\nğŸ¯ å…³é”®ä¼˜åŠ¿:")
print(f"   1. ä½¿ç”¨äº†å…¨éƒ¨ {len(ALL_SEGMENTS_WITH_FEATURES)} ä¸ªç‰‡æ®µè¿›è¡Œæµ‹è¯•")
print(f"   2. æ¯ä¸ªç‰‡æ®µéƒ½æœ‰ä¸€æ¬¡æµ‹è¯•æœºä¼šï¼ˆåœ¨å¯¹åº”çš„ fold ä¸­ï¼‰")
print(f"   3. å¯ä»¥åœ¨å…¨éƒ¨æ•°æ®ä¸Šå¯¹æ¯”æ‰‹å·¥é€‰æ‹©ã€RFæ— çº¦æŸã€RFçº¦æŸ")
print(f"\nğŸ“Š å…¨å±€æ€§èƒ½:")
print(f"   Range Bin å‡†ç¡®ç‡: {global_rb_results['correct'].mean():.2%}")
print(f"   å¹³å‡è·ç¦»è¯¯å·®: {global_rb_results['error'].mean():.2f} Bins")
print(f"\nğŸ“ è¾“å‡ºæ–‡ä»¶:")
print(f"   - fold_summary_*.csv: å„ fold æ€§èƒ½æ±‡æ€»")
print(f"   - global_rb_results_*.csv: å…¨å±€ Range Bin é¢„æµ‹ç»“æœ")
print(f"   - all_predictions_*.csv: æ‰€æœ‰é¢„æµ‹è¯¦æƒ…ï¼ˆç”¨äºå¿ƒç‡ä¼°è®¡ï¼‰")
print(f"   - rf_model_fold*_*.pkl: å„ fold çš„æ¨¡å‹")
print("=" * 70)

