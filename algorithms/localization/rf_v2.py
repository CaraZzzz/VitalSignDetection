"""
Random Forestæ–¹æ³•äººä½“å®šä½ - æ”¹è¿›ç‰ˆï¼ˆé™åˆ¶é€‰æ‹©èŒƒå›´ï¼‰
ç›¸æ¯”v1ï¼Œé™åˆ¶äº†range bin é€‰æ‹©èŒƒå›´
ä½¿ç”¨ç®€åŒ–ç‰ˆç‰¹å¾æå–
"""
import numpy as np
import joblib
import json
import os
from typing import Dict, Any, Tuple
from scipy import stats
from .base_localization import BaseLocalizationMethod


class RandomForestLocalization(BaseLocalizationMethod):
    """
    åŸºäºéšæœºæ£®æ—æ¨¡å‹çš„äººä½“å®šä½æ–¹æ³•ï¼ˆå¸¦èŒƒå›´çº¦æŸï¼‰
    
    ä½¿ç”¨é¢„è®­ç»ƒçš„éšæœºæ£®æ—æ¨¡å‹é¢„æµ‹äººä½“æ‰€åœ¨çš„Range Bin
    ä¸ºé˜²æ­¢é€‰æ‹©åˆ°è¿œè·ç¦»é”™è¯¯çš„Binï¼Œé™åˆ¶é€‰æ‹©èŒƒå›´åœ¨ Bin 4-10
    """
    
    def __init__(self, model_path: str, scaler_path: str, metadata_path: str,
                 valid_bin_range: Tuple[int, int] = (4, 10)):
        """
        åˆå§‹åŒ–Random Forestå®šä½æ–¹æ³•
        
        å‚æ•°:
            model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„ï¼ˆ.pklï¼‰- ä½¿ç”¨ç»å¯¹è·¯å¾„
            scaler_path: æ ‡å‡†åŒ–å™¨æ–‡ä»¶è·¯å¾„ï¼ˆ.pklï¼‰- ä½¿ç”¨ç»å¯¹è·¯å¾„
            metadata_path: å…ƒæ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆ.jsonï¼‰- ä½¿ç”¨ç»å¯¹è·¯å¾„
            valid_bin_range: æœ‰æ•ˆçš„Range BinèŒƒå›´ï¼ˆbase-1ï¼‰ï¼Œé»˜è®¤ (4, 10)
        """
        super().__init__()
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶æœªæ‰¾åˆ°: {model_path}")
        if not os.path.exists(scaler_path):
            raise FileNotFoundError(f"æ ‡å‡†åŒ–å™¨æ–‡ä»¶æœªæ‰¾åˆ°: {scaler_path}")
        if not os.path.exists(metadata_path):
            raise FileNotFoundError(f"å…ƒæ•°æ®æ–‡ä»¶æœªæ‰¾åˆ°: {metadata_path}")
        
        # åŠ è½½æ¨¡å‹
        self.model = joblib.load(model_path)
        self.scaler = joblib.load(scaler_path)
        
        with open(metadata_path, 'r', encoding='utf-8') as f:
            self.metadata = json.load(f)
        
        # è®¾ç½®æœ‰æ•ˆèŒƒå›´ï¼ˆbase-1 è½¬æ¢ä¸º base-0ï¼‰
        self.valid_bin_min = valid_bin_range[0] - 1  # base-0
        self.valid_bin_max = valid_bin_range[1] - 1  # base-0
        
        print(f"âœ… æˆåŠŸåŠ è½½éšæœºæ£®æ—æ¨¡å‹: {os.path.basename(model_path)}")
        print(f"   æµ‹è¯•é›†AUC: {self.metadata.get('test_auc', 'N/A')}")
        print(f"   æµ‹è¯•é›†F1: {self.metadata.get('test_f1', 'N/A')}")
        print(f"   ğŸ¯ Range Bin é€‰æ‹©èŒƒå›´é™åˆ¶: Bin {valid_bin_range[0]}-{valid_bin_range[1]} (base-1)")
    
    def _extract_features(self, segment_data: Dict[str, Any]) -> np.ndarray:
        """
        æå–ç‰¹å¾
        
        å‚æ•°:
            segment_data: ç‰‡æ®µæ•°æ®å­—å…¸
        
        è¿”å›:
            features: (R x nVX x n_features) ç‰¹å¾æ•°ç»„
        """
        magnitude = segment_data['magnitude']
        unfiltered_phase = segment_data['unfiltered_phase']
        
        R, nVX, T = magnitude.shape
        feature_names = self.metadata['feature_names']
        n_features = len(feature_names)
        
        features = np.zeros((R, nVX, n_features))
        
        for r in range(R):
            for v in range(nVX):
                amp_ts = magnitude[r, v, :]
                phase_ts = unfiltered_phase[r, v, :]
                
                feature_vector = []
                
                for feat_name in feature_names:
                    if 'amp_mean' in feat_name:
                        feature_vector.append(np.mean(amp_ts))
                    elif 'amp_std' in feat_name:
                        feature_vector.append(np.std(amp_ts))
                    elif 'amp_p2p' in feat_name:
                        feature_vector.append(np.max(amp_ts) - np.min(amp_ts))
                    elif 'amp_skewness' in feat_name:
                        feature_vector.append(stats.skew(amp_ts))
                    elif 'amp_kurtosis' in feat_name:
                        feature_vector.append(stats.kurtosis(amp_ts))
                    elif 'phase_diff_std' in feat_name:
                        phase_diff = np.diff(phase_ts)
                        feature_vector.append(np.std(phase_diff))
                    elif 'phase_diff_range' in feat_name:
                        phase_diff = np.diff(phase_ts)
                        feature_vector.append(np.max(phase_diff) - np.min(phase_diff))
                    elif 'amp_snr_db' in feat_name or 'phase_snr_db' in feat_name:
                        feature_vector.append(10.0)
                    else:
                        feature_vector.append(0.0)
                
                features[r, v, :] = feature_vector
        
        return features
    
    def select_range_bin(self, segment_data: Dict[str, Any], 
                        predefined_rb_index: int = None) -> Tuple[int, Dict[str, Any]]:
        """
        ä½¿ç”¨éšæœºæ£®æ—æ¨¡å‹é€‰æ‹©Range Binï¼ˆé™åˆ¶åœ¨åˆç†èŒƒå›´å†…ï¼‰
        
        å‚æ•°:
            segment_data: ç‰‡æ®µæ•°æ®å­—å…¸
            predefined_rb_index: é¢„å®šä¹‰çš„Range Binç´¢å¼•ï¼ˆæœªä½¿ç”¨ï¼Œä¸ºäº†ä¿æŒæ¥å£ä¸€è‡´ï¼‰
        
        è¿”å›:
            (selected_rb_index, selection_info)
        """
        # æå–ç‰¹å¾
        features = self._extract_features(segment_data)
        
        # è·å–RXç´¢å¼•
        rx_index = segment_data['rx_index']  # 1-based
        rx_idx_0based = rx_index - 1
        
        R = features.shape[0]
        
        # æå–è¯¥RXçš„ç‰¹å¾å¹¶æ ‡å‡†åŒ–
        features_flat = features[:, rx_idx_0based, :]  # (R x n_features)
        features_scaled = self.scaler.transform(features_flat)
        
        # é¢„æµ‹æ¦‚ç‡
        probabilities = self.model.predict_proba(features_scaled)[:, 1]  # äººä½“ç±»åˆ«çš„æ¦‚ç‡
        
        # ğŸ”¥ å…³é”®æ”¹è¿›ï¼šé™åˆ¶é€‰æ‹©èŒƒå›´
        # åˆ›å»ºæ©ç æ¦‚ç‡æ•°ç»„ï¼ˆåªä¿ç•™æœ‰æ•ˆèŒƒå›´å†…çš„æ¦‚ç‡ï¼‰
        masked_probabilities = np.zeros_like(probabilities)
        masked_probabilities[self.valid_bin_min:self.valid_bin_max+1] = \
            probabilities[self.valid_bin_min:self.valid_bin_max+1]
        
        # åœ¨æœ‰æ•ˆèŒƒå›´å†…é€‰æ‹©æ¦‚ç‡æœ€é«˜çš„Range Bin
        if np.max(masked_probabilities) > 0:
            # æœ‰æ•ˆèŒƒå›´å†…æœ‰é¢„æµ‹
            predicted_rb_0based = np.argmax(masked_probabilities)
            predicted_confidence = masked_probabilities[predicted_rb_0based]
            selection_strategy = 'constrained_prediction'
        else:
            # å¦‚æœæœ‰æ•ˆèŒƒå›´å†…æ²¡æœ‰é¢„æµ‹ï¼ˆç†è®ºä¸Šä¸åº”è¯¥å‘ç”Ÿï¼‰ï¼Œä½¿ç”¨åŸå§‹é¢„æµ‹
            predicted_rb_0based = np.argmax(probabilities)
            predicted_confidence = probabilities[predicted_rb_0based]
            selection_strategy = 'unconstrained_fallback'
            print(f"âš ï¸ è­¦å‘Šï¼šæ‰€æœ‰æœ‰æ•ˆ Bin ({self.valid_bin_min+1}-{self.valid_bin_max+1}) "
                  f"çš„æ¦‚ç‡ä¸º 0ï¼Œä½¿ç”¨æœªçº¦æŸé¢„æµ‹")
        
        # è®°å½•åŸå§‹é¢„æµ‹ï¼ˆæœªçº¦æŸï¼‰
        original_max_bin_0based = np.argmax(probabilities)
        
        selection_info = {
            'method': 'random_forest_constrained',
            'confidence': float(predicted_confidence),
            'probabilities': probabilities.tolist(),
            'masked_probabilities': masked_probabilities.tolist(),
            'valid_range': f'Bin {self.valid_bin_min+1}-{self.valid_bin_max+1} (base-1)',
            'selection_strategy': selection_strategy,
            'original_max_bin': int(original_max_bin_0based) + 1,  # base-1 ç”¨äºè®°å½•
            'selected_bin': int(predicted_rb_0based) + 1,  # base-1 ç”¨äºè®°å½•
            'was_constrained': bool(original_max_bin_0based != predicted_rb_0based)
        }
        
        return predicted_rb_0based, selection_info
    
    def get_method_name(self) -> str:
        """è·å–æ–¹æ³•åç§°"""
        return "RandomForest_Constrained"